{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: \n",
    "- Import df\n",
    "- df = 'url', 'name', 'price', 'color', 'images', 'image link'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://www.asos.com/new-look/new-look-trench-...   \n",
      "1  https://www.asos.com/stradivarius/stradivarius...   \n",
      "2  https://www.asos.com/jdy/jdy-oversized-trench-...   \n",
      "3  https://www.asos.com/nike-running/nike-running...   \n",
      "4  https://www.asos.com/asos-curve/asos-design-cu...   \n",
      "\n",
      "                                                name  price    color  \\\n",
      "0                      New Look trench coat in camel  49.99  Neutral   \n",
      "1     Stradivarius double breasted wool coat in grey  59.99     GREY   \n",
      "2                 JDY oversized trench coat in stone  45.00    STONE   \n",
      "3                 Nike Running hooded jacket in pink  84.95     Pink   \n",
      "4  ASOS DESIGN Tall linen mix trench coat in natural  75.00  Natural   \n",
      "\n",
      "                                              images  \\\n",
      "0  ['https://images.asos-media.com/products/new-l...   \n",
      "1  ['https://images.asos-media.com/products/strad...   \n",
      "2  ['https://images.asos-media.com/products/jdy-o...   \n",
      "3  ['https://images.asos-media.com/products/nike-...   \n",
      "4  ['https://images.asos-media.com/products/asos-...   \n",
      "\n",
      "                                          image link  \n",
      "0  https://images.asos-media.com/products/new-loo...  \n",
      "1  https://images.asos-media.com/products/stradiv...  \n",
      "2  https://images.asos-media.com/products/jdy-ove...  \n",
      "3  https://images.asos-media.com/products/nike-ru...  \n",
      "4  https://images.asos-media.com/products/asos-de...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CAREFUL WHEN REDUCING:\n",
    "#       Need to delete faiss_index.bin \n",
    "# downloaded = 500\n",
    "num_samples = 100\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"products.csv\").head(num_samples)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['url', 'name', 'price', 'color', 'images', 'image link']]\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: (takes time)\n",
    "- Download image\n",
    "- Extract description\n",
    "- Generate description\n",
    "- Apply LLaVa\n",
    "- df = 'url', 'name', 'price', 'color', 'images', 'image link' + description, image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image already exists: ./images_data/New_Look_trench_coat_in_camel.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_trench_coat_in_camel.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Neutral New Look trench coat in camel in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Neutral New Look trench coat in camel.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   20832.31 ms\n",
      "llama_perf_context_print: prompt eval time =   18935.35 ms /  3165 tokens (    5.98 ms per token,   167.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6167.83 ms /   136 runs   (   45.35 ms per token,    22.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   27067.45 ms /  3301 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look trench coat in camel: The trench coat in the image is a Neutral New Look trench coat in camel. It's a classic piece with a versatile style, featuring a belted waist, epaulettes, and a wide lapel. The fabric appears to be a soft cotton blend, giving it a comfortable and breathable feel. The trench coat's length and relaxed fit make it suitable for a variety of occasions, from casual to formal. It pairs well with both jeans and high heels, and is ideal for business attire. The coat's neutral color makes it suitable for different weather conditions, from mild spring days to rainy autumn days.\n",
      "Image already exists: ./images_data/Stradivarius_double_breasted_wool_coat_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_double_breasted_wool_coat_in_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY Stradivarius double breasted wool coat in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY Stradivarius double breasted wool coat in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15287.58 ms\n",
      "llama_perf_context_print: prompt eval time =   13837.00 ms /  3171 tokens (    4.36 ms per token,   229.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6295.59 ms /   142 runs   (   44.34 ms per token,    22.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   21640.71 ms /  3313 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius double breasted wool coat in grey: The grey Stradivarius double breasted wool coat in the image is a stylish, versatile piece that stands out with its unique design. Its fabric appears soft and comfortable, suggesting a high level of wearability. The coat's loose fit allows for a good range of motion while still maintaining a fashionable look. Made of wool, it offers excellent insulation, making it an ideal choice for colder weather conditions. This coat would be best suited for casual events, date nights, and business attire. It pairs well with a variety of outfits, from jeans and sneakers to dress pants and high heels. Overall, this grey coat offers a sophisticated, yet practical style.\n",
      "Image already exists: ./images_data/JDY_oversized_trench_coat_in_stone.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/JDY_oversized_trench_coat_in_stone.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the STONE JDY oversized trench coat in stone in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the STONE JDY oversized trench coat in stone.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14572.73 ms\n",
      "llama_perf_context_print: prompt eval time =   13106.59 ms /  3167 tokens (    4.14 ms per token,   241.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5557.04 ms /   128 runs   (   43.41 ms per token,    23.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   20181.29 ms /  3295 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for JDY oversized trench coat in stone: The image features a person wearing a beige-colored trench coat. This trench coat is oversized, which gives it a relaxed and comfortable fit. The fabric appears to be a soft and lightweight material, suggesting that it is breathable and suitable for a range of weather conditions. The coat's style is classic and timeless, with its wide lapels and roomy fit. It pairs well with both casual and more formal attire, making it versatile and suitable for various occasions. The person is seen in the background, wearing blue jeans and tan boots, which complement the beige trench coat well.\n",
      "Image already exists: ./images_data/Nike_Running_hooded_jacket_in_pink.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Nike_Running_hooded_jacket_in_pink.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Pink Nike Running hooded jacket in pink in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Pink Nike Running hooded jacket in pink.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14381.04 ms\n",
      "llama_perf_context_print: prompt eval time =   12881.24 ms /  3161 tokens (    4.08 ms per token,   245.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7988.35 ms /   184 runs   (   43.41 ms per token,    23.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   22425.94 ms /  3345 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Nike Running hooded jacket in pink: The Pink Nike Running hooded jacket in the image is a vibrant and stylish choice. It's made of a lightweight and breathable fabric, perfect for casual outings or a light jog on a cool day. The jacket's design offers a relaxed fit, ensuring comfort while running. The hood is a practical addition, offering protection from the elements. The jacket's overall aesthetic is sporty and chic, making it an excellent option for both casual and semi-formal occasions. The Nike brand's logo on the jacket adds a touch of sophistication to the garment. Pairing the jacket with jeans and sneakers creates a casual look, while dressing it up with high heels or a trench coat can elevate it for more formal occasions. The jacket's design and material make it suitable for a variety of weather conditions, from cool to mild temperatures.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Tall_linen_mix_trench_coat_in_natural.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Tall_linen_mix_trench_coat_in_natural.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Natural ASOS DESIGN Tall linen mix trench coat in natural in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Natural ASOS DESIGN Tall linen mix trench coat in natural.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14443.62 ms\n",
      "llama_perf_context_print: prompt eval time =   12980.40 ms /  3171 tokens (    4.09 ms per token,   244.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6252.31 ms /   144 runs   (   43.42 ms per token,    23.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   20750.15 ms /  3315 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Tall linen mix trench coat in natural: In the image, a woman is stylishly dressed in a beige-colored Natural ASOS DESIGN Tall linen mix trench coat. The coat features a lightweight fabric that is both soft and comfortable, making it ideal for various occasions. Its loose fit and relaxed style contribute to its overall aesthetic, which is elegant and chic. The coat's trench design and beige color make it suitable for both casual and formal events, and it pairs well with both jeans and high heels. The length of the coat suggests that it would provide sufficient coverage in cooler weather conditions, while the linen material and lightweight fabric make it breathable and suitable for spring and summer weather.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_denim_bomber_in_ecru.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_denim_bomber_in_ecru.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the ECRU ASOS DESIGN denim bomber in ecru in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the ECRU ASOS DESIGN denim bomber in ecru.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14706.80 ms\n",
      "llama_perf_context_print: prompt eval time =   13213.43 ms /  3171 tokens (    4.17 ms per token,   239.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6641.27 ms /   152 runs   (   43.69 ms per token,    22.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   21401.77 ms /  3323 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN denim bomber in ecru: The ECRU ASOS DESIGN denim bomber in the image is a versatile piece of clothing that exudes a casual and relaxed style. The denim fabric is soft and comfortable, with a light, ecru hue that gives it a chic and sophisticated look. The bomber style features a zippered front and long sleeves, adding a sense of ruggedness to the piece. The denim fabric is both breathable and durable, making it suitable for various weather conditions. It pairs well with jeans for a laid-back look or high heels for a more formal occasion. The bomber jacket is a great addition to any wardrobe, adding a touch of elegance to both casual and formal outfits.\n",
      "Image already exists: ./images_data/ASOS_Weekend_Collective_nylon_track_jacket_in_neutral.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_Weekend_Collective_nylon_track_jacket_in_neutral.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the NEUTRAL ASOS Weekend Collective nylon track jacket in neutral in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the NEUTRAL ASOS Weekend Collective nylon track jacket in neutral.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14501.73 ms\n",
      "llama_perf_context_print: prompt eval time =   13006.49 ms /  3175 tokens (    4.10 ms per token,   244.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7921.18 ms /   177 runs   (   44.75 ms per token,    22.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   22484.34 ms /  3352 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS Weekend Collective nylon track jacket in neutral: The ASOS Weekend Collective nylon track jacket in neutral, as seen in the image, is a versatile and stylish piece of clothing. The nylon fabric gives it a lightweight and breathable quality, perfect for casual wear or outdoor activities. The neutral color palette allows it to easily blend with other clothing items, making it a versatile piece to have in one's wardrobe. Its loose fit ensures comfort, while the ribbed cuffs and hem provide a snug fit for the wearer. The material's texture is smooth and soft, making it a pleasure to wear. The jacket's design is casual, yet chic, making it suitable for various occasions such as a weekend get-together or a casual date. It pairs well with jeans and sneakers, adding a touch of sportiness to any outfit.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Tall_ultimate_faux_leather_biker_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Tall_ultimate_faux_leather_biker_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Tall ultimate faux leather biker jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Tall ultimate faux leather biker jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   16171.20 ms\n",
      "llama_perf_context_print: prompt eval time =   14639.89 ms /  3173 tokens (    4.61 ms per token,   216.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8959.38 ms /   208 runs   (   43.07 ms per token,    23.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   25187.94 ms /  3381 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Tall ultimate faux leather biker jacket in black: The Black ASOS DESIGN Tall ultimate faux leather biker jacket in black is a striking piece of clothing that immediately catches the eye. It's made of a high-quality, faux leather material that gives it an authentic, biker-inspired look. The fabric is sturdy and seems to offer a good level of comfort and mobility. Despite being a tall jacket, it doesn't feel restrictive, allowing for a full range of movement. The texture of the material is a rich, glossy black that suggests it's suitable for various weather conditions. The design is versatile and can be worn for casual outings, formal events, or even on a date night. Pairing it with jeans and sneakers could give a casual look, while pairing it with high heels and a trench coat can elevate it to a more formal setting. The overall aesthetic of the jacket is both stylish and elegant, making it a great addition to anyone's wardrobe.\n",
      "Image already exists: ./images_data/Native_Youth_oversized_twill_shacket_co-ord_in_purple.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Native_Youth_oversized_twill_shacket_co-ord_in_purple.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the PURPLE Native Youth oversized twill shacket co-ord in purple in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the PURPLE Native Youth oversized twill shacket co-ord in purple.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14597.16 ms\n",
      "llama_perf_context_print: prompt eval time =   13103.16 ms /  3175 tokens (    4.13 ms per token,   242.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6061.34 ms /   141 runs   (   42.99 ms per token,    23.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   20711.06 ms /  3316 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Native Youth oversized twill shacket co-ord in purple: The image showcases a vibrant purple Native Youth oversized twill shacket co-ord, featuring a harmonious blend of style and functionality. The fabric is lightweight and breathable, providing a comfortable fit and a high degree of mobility. The material has a soft texture, which makes it suitable for various weather conditions, from casual wear to more formal occasions. Pairing this shacket with jeans or sneakers would complement its casual aesthetic, while combining it with high heels or a trench coat could elevate its style for more formal events or outdoor wear. The overall aesthetic of the shacket is both fashionable and practical, making it an ideal choice for various occasions.\n",
      "Image already exists: ./images_data/Carhartt_WIP_michigan_OG_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Carhartt_WIP_michigan_OG_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Carhartt WIP michigan OG jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Carhartt WIP michigan OG jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14852.48 ms\n",
      "llama_perf_context_print: prompt eval time =   13347.27 ms /  3175 tokens (    4.20 ms per token,   237.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9193.68 ms /   210 runs   (   43.78 ms per token,    22.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   24102.50 ms /  3385 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Carhartt WIP michigan OG jacket in black: The black Carhartt WIP michigan OG jacket in the image is a classic piece of outerwear. Its fabric is a rich, textured canvas that suggests durability and style. The style is reminiscent of a traditional work jacket, featuring a high collar and a button-up front. The jacket is adorned with gold buttons, adding a touch of elegance to the rugged design. The material of the jacket appears to be soft and flexible, providing a comfortable wear. Despite its robust appearance, the jacket seems to have a relaxed fit, offering ample space for movement and a casual feel. The overall aesthetic of the jacket is versatile, making it suitable for various occasions, from casual outings to outdoor adventures. It pairs well with both high heels and sneakers, and it adds a stylish touch to both jeans and trench coats. This jacket is ideal for various weather conditions, ranging from mild to chilly, and can be worn in both casual and formal settings.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_denim_short_sleeve_shirt_in_midwash_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_denim_short_sleeve_shirt_in_midwash_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLUE ASOS DESIGN denim short sleeve shirt in midwash blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLUE ASOS DESIGN denim short sleeve shirt in midwash blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14536.74 ms\n",
      "llama_perf_context_print: prompt eval time =   12968.78 ms /  3175 tokens (    4.08 ms per token,   244.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6356.55 ms /   149 runs   (   42.66 ms per token,    23.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   20946.70 ms /  3324 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN denim short sleeve shirt in midwash blue: The BLUE ASOS DESIGN denim short sleeve shirt in midwash blue is a classic, versatile piece. It is crafted from a sturdy yet comfortable denim fabric, giving it a slightly distressed appearance that adds to its aesthetic appeal. The style is relaxed and casual, with a regular fit that allows for ease of movement. The material, a blend of cotton and spandex, provides both durability and a touch of stretch for added comfort. The shirt is suitable for various weather conditions, from casual summer days to cooler evenings. It pairs well with both jeans and trousers, making it a versatile option for a range of occasions from casual to semi-formal.\n",
      "Image already exists: ./images_data/Bershka_maxi_belted_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_maxi_belted_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Bershka maxi belted coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Bershka maxi belted coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   13994.19 ms\n",
      "llama_perf_context_print: prompt eval time =   12508.94 ms /  3169 tokens (    3.95 ms per token,   253.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10428.98 ms /   242 runs   (   43.09 ms per token,    23.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   24481.50 ms /  3411 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka maxi belted coat in black: The black Bershka maxi belted coat in the image is a striking piece of outerwear. The fabric appears to be a soft, smooth suede, which gives the coat a luxurious and comfortable feel. The style of the coat is reminiscent of a robe, with a belted waist and wide lapels that add a touch of elegance to the overall look. The coat's design also features large pockets, perfect for keeping hands warm and adding a practical element to the aesthetic. The belted waist is a noticeable feature, suggesting a tailored fit that is both stylish and functional. The material and texture of the coat suggest it's suitable for a variety of weather conditions, providing both warmth and style. Its versatile design makes it best suited for casual outings, date nights, and even business attire when paired with the right accessories. The coat's belted waist can be cinched for a more fitted look, while its wide lapels can be left open for a more relaxed, comfortable fit. Overall, the black Bershka maxi belted coat is a perfect blend of style, comfort, and practicality.\n",
      "Image already exists: ./images_data/Vero_Moda_trench_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Vero_Moda_trench_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Vero Moda trench coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Vero Moda trench coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14444.46 ms\n",
      "llama_perf_context_print: prompt eval time =   12918.90 ms /  3165 tokens (    4.08 ms per token,   244.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6966.34 ms /   160 runs   (   43.54 ms per token,    22.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   21467.31 ms /  3325 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Vero Moda trench coat in black: The black Vero Moda trench coat in this image is a stylish and versatile piece, perfect for various weather conditions and occasions. Made of a soft cotton material, it exudes comfort and warmth while maintaining a lightweight feel. The coat's cuffed sleeves and belted waist offer an adjustable fit, catering to different body types and preferences. The overall aesthetic is classic and timeless, featuring a double-breasted design and a high collar that adds an element of sophistication. The trench coat is ideal for rainy days, providing waterproof protection, while its breathable fabric makes it suitable for summer wear. For casual outings, pair it with jeans and sneakers, or opt for high heels and tights for a more formal look.\n",
      "Image already exists: ./images_data/Stradivarius_tailored_coat_in_beige.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_tailored_coat_in_beige.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BEIGE Stradivarius tailored coat in beige in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BEIGE Stradivarius tailored coat in beige.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14428.90 ms\n",
      "llama_perf_context_print: prompt eval time =   12890.43 ms /  3169 tokens (    4.07 ms per token,   245.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6287.19 ms /   144 runs   (   43.66 ms per token,    22.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   20770.62 ms /  3313 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius tailored coat in beige: In the image, a young woman is seen modeling a beige Stradivarius tailored coat. The coat, which is the central focus of the image, is crafted from a soft, lightweight fabric that provides a comfortable and cozy fit. The material is flexible, allowing for ease of movement and breathability. Its style is versatile, making it suitable for various weather conditions, from casual outings to formal events. The coat pairs well with a variety of clothing items, complementing both denim jeans and high heels. Its light beige color adds a touch of elegance to any outfit, making it a perfect choice for a range of occasions, from date nights to business meetings.\n",
      "Image already exists: ./images_data/Stradivarius_tailored_belted_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_tailored_belted_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Stradivarius tailored belted coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Stradivarius tailored belted coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14567.10 ms\n",
      "llama_perf_context_print: prompt eval time =   13083.45 ms /  3169 tokens (    4.13 ms per token,   242.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7087.77 ms /   163 runs   (   43.48 ms per token,    23.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   21710.97 ms /  3332 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius tailored belted coat in black: The BLACK Stradivarius tailored belted coat in the image exudes a sense of sophistication and style. The fabric appears to be a smooth, luxurious material, possibly wool or a wool blend, which gives the coat a soft and comfortable texture. The tailored fit suggests a relaxed yet elegant style, allowing for ease of movement while still maintaining a stylish appearance. The belted design adds a touch of formality to the overall aesthetic, making the coat ideal for colder weather conditions. Its design is suitable for various occasions, from casual gatherings to more formal events such as date nights or business meetings. Pairing the coat with jeans and sneakers can create a casual look, while high heels and a trench coat can elevate it for a more sophisticated appearance.\n",
      "Image already exists: ./images_data/Miss_Selfridge_longline_faux_leather_biker_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Miss_Selfridge_longline_faux_leather_biker_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Miss Selfridge longline faux leather biker jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Miss Selfridge longline faux leather biker jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14497.69 ms\n",
      "llama_perf_context_print: prompt eval time =   13028.69 ms /  3173 tokens (    4.11 ms per token,   243.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9734.04 ms /   224 runs   (   43.46 ms per token,    23.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   24290.35 ms /  3397 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Miss Selfridge longline faux leather biker jacket in black: The Miss Selfridge longline faux leather biker jacket in black is a fashionable piece that exudes a chic, edgy vibe. The fabric appears to be of a high-quality material, which is likely to provide a comfortable fit. The style is versatile, featuring a relaxed yet stylish fit that would work well with various bottoms and footwear. The pattern is simple yet striking, with a shiny finish that gives the jacket a luxurious feel. Despite the faux leather material, the jacket seems to be quite breathable, making it suitable for a range of weather conditions. In terms of occasions, it could be ideal for casual events, date nights, or even a touch of edginess for business attire. Pairing it with ripped jeans and sneakers would complete the outfit, while a trench coat and high heels could elevate the look for more formal occasions. Overall, this black Miss Selfridge longline faux leather biker jacket is a versatile and fashionable piece that can easily become a staple in any wardrobe.\n",
      "Image already exists: ./images_data/Bershka_faux_leather_motorcross_jacket_in_light_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_faux_leather_motorcross_jacket_in_light_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY Bershka faux leather motorcross jacket in light grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY Bershka faux leather motorcross jacket in light grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14526.43 ms\n",
      "llama_perf_context_print: prompt eval time =   12989.43 ms /  3175 tokens (    4.09 ms per token,   244.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8037.65 ms /   185 runs   (   43.45 ms per token,    23.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   22619.42 ms /  3360 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka faux leather motorcross jacket in light grey: The GREY Bershka faux leather motorcross jacket in light grey is a stylish and trendy piece of clothing. It's made of a soft and comfortable fabric that resembles leather, but it's actually a lightweight, breathable material. The jacket's design is edgy yet sophisticated, perfect for casual outings or a night out with friends. The fit of the jacket is relaxed, allowing for a comfortable and unrestricted movement. The style features a blend of traditional motorcross jacket elements with a modern touch, making it suitable for various weather conditions, from mild to cooler temperatures. Pairing the jacket with black pants or jeans would complete the look, while sneakers or boots could add a touch of ruggedness. It's a versatile piece that could easily transition from day to night or be dressed up or down depending on the occasion.\n",
      "Image already exists: ./images_data/Bershka_oversized_bomber_puffer_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_oversized_bomber_puffer_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Bershka oversized bomber puffer jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Bershka oversized bomber puffer jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14770.82 ms\n",
      "llama_perf_context_print: prompt eval time =   13265.45 ms /  3173 tokens (    4.18 ms per token,   239.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6684.24 ms /   153 runs   (   43.69 ms per token,    22.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   21509.50 ms /  3326 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka oversized bomber puffer jacket in black: The image presents a chic and stylish woman modeling the black Bershka oversized bomber puffer jacket. The jacket exudes a sense of comfort and warmth, featuring a loose fit that allows for easy movement. The material appears soft and plush, suitable for both casual and cool weather conditions. Paired with her blue jeans, the jacket creates a striking contrast, making it an ideal choice for casual outings, date nights, and even outdoor activities. The oversized fit and puffed sleeves give the jacket a fluffy and cozy texture, making it a perfect accessory for those colder days. The bomber-style design adds a touch of edginess, while the overall aesthetic is both fashionable and functional.\n",
      "Image already exists: ./images_data/Topshop_denim_balloon_jacket_in_dirty_dark_wash.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_denim_balloon_jacket_in_dirty_dark_wash.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Dirty Dark Wash Topshop denim balloon jacket in dirty dark wash in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Dirty Dark Wash Topshop denim balloon jacket in dirty dark wash.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14488.89 ms\n",
      "llama_perf_context_print: prompt eval time =   12995.51 ms /  3175 tokens (    4.09 ms per token,   244.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8545.91 ms /   191 runs   (   44.74 ms per token,    22.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   23100.22 ms /  3366 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop denim balloon jacket in dirty dark wash: The Dirty Dark Wash Topshop denim balloon jacket in dirty dark wash is a versatile piece that exudes a cool, urban vibe. The fabric is soft and pliable, suggesting a comfortable fit that moves with the wearer. The jacket features a unique balloon shape, adding a touch of whimsy to its design. The dirty dark wash pattern is subtle and doesn't overpower the overall aesthetic, making the jacket a versatile piece that can be paired with a variety of outfits. The loose fit and relaxed style of the jacket make it ideal for casual wear or laid-back weekend get-togethers. Its breathable fabric and mobility-friendly design make it suitable for warm weather and outdoor events. The Dirty Dark Wash Topshop denim balloon jacket can be paired with both casual and smart attire, making it a great choice for a range of occasions.\n",
      "Image already exists: ./images_data/Barbour_Annandale_diamond_quilt_jacket_with_cord_collar_in_navy.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Barbour_Annandale_diamond_quilt_jacket_with_cord_collar_in_navy.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Navy Barbour Annandale diamond quilt jacket with cord collar in navy in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Navy Barbour Annandale diamond quilt jacket with cord collar in navy.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14677.68 ms\n",
      "llama_perf_context_print: prompt eval time =   13264.99 ms /  3173 tokens (    4.18 ms per token,   239.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9185.42 ms /   213 runs   (   43.12 ms per token,    23.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   23920.75 ms /  3386 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Barbour Annandale diamond quilt jacket with cord collar in navy: In the image, a woman is standing against a white background, showcasing a navy blue Barbour Annandale diamond quilt jacket with a cord collar. The jacket is the focal point of the image, its rich, dark navy blue color contrasting with the white background. The quilted pattern adds texture and depth to the jacket, making it both stylish and functional. The fit of the jacket appears to be relaxed, providing a comfortable and cozy feel. The material of the jacket appears to be a blend of cotton and wool, offering a balance of softness and warmth. Given its quilted pattern and material, this jacket is suitable for a range of weather conditions, from cool spring evenings to crisp autumn mornings. It would pair well with denim jeans and white sneakers for a casual look, or with high heels and a skirt for a more formal occasion. It's versatile enough to be worn to a date night or for business attire, while still being comfortable enough for outdoor wear.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_borg_denim_jacket_in_washed_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_borg_denim_jacket_in_washed_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the WASHED BLUE ASOS DESIGN borg denim jacket in washed blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the WASHED BLUE ASOS DESIGN borg denim jacket in washed blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14516.85 ms\n",
      "llama_perf_context_print: prompt eval time =   12997.12 ms /  3177 tokens (    4.09 ms per token,   244.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8242.77 ms /   191 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   22815.65 ms /  3368 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN borg denim jacket in washed blue: The WASHED BLUE ASOS DESIGN borg denim jacket in washed blue is a stylish and comfortable piece of clothing. Its fabric is soft and plush, with the blue denim adding a casual touch to the overall look. The style is relaxed and laid-back, with a fluffy texture that provides a sense of warmth. The wash gives it a lived-in, worn look, making it ideal for everyday wear. The fit is loose yet well-tailored, allowing for a comfortable and unrestricted mobility. Its material and texture make it suitable for a variety of weather conditions, from spring to fall. When it comes to pairing, this jacket can be worn with jeans, sneakers, or high heels, and would also look great with a trench coat for a more formal occasion. It's versatile and can be dressed up or down, making it a perfect addition to any wardrobe.\n",
      "Image already exists: ./images_data/Vero_Moda_hooded_rain_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Vero_Moda_hooded_rain_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Vero Moda hooded rain jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Vero Moda hooded rain jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15009.06 ms\n",
      "llama_perf_context_print: prompt eval time =   13469.73 ms /  3167 tokens (    4.25 ms per token,   235.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7359.57 ms /   170 runs   (   43.29 ms per token,    23.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   22422.92 ms /  3337 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Vero Moda hooded rain jacket in black: The BLACK Vero Moda hooded rain jacket in black is a stylish piece of outerwear. Crafted from a waterproof material, it provides a high level of protection against the elements, making it perfect for rainy days. The fit is relaxed and comfortable, allowing for a range of mobility. The fabric of the jacket is smooth and sleek, enhancing its aesthetic appeal. The overall style of the jacket is modern and chic, suitable for various occasions ranging from casual outings to more formal events. It pairs well with jeans or sneakers for a casual look, or with high heels and a trench coat for a more formal occasion. The BLACK Vero Moda hooded rain jacket in black is a versatile and functional garment that is sure to become a staple in any wardrobe.\n",
      "Image already exists: ./images_data/New_Look_Tall_trench_coat_in_camel.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_Tall_trench_coat_in_camel.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Camel New Look Tall trench coat in camel in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Camel New Look Tall trench coat in camel.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14209.04 ms\n",
      "llama_perf_context_print: prompt eval time =   12752.04 ms /  3165 tokens (    4.03 ms per token,   248.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8301.40 ms /   191 runs   (   43.46 ms per token,    23.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   22567.82 ms /  3356 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look Tall trench coat in camel: The Camel New Look Tall trench coat in camel exhibits a classic and elegant style. Its fabric appears to be a medium-weight, possibly a wool or wool blend, which gives it a luxurious texture. The coat is designed with a double-breasted style, featuring large buttons and a belt at the waist, which adds to its formal aesthetic. Its overall fit seems to be loose yet comfortable, providing ample room for movement. This coat is suitable for various weather conditions, offering warmth and protection from the elements, while its breathable fabric allows for some comfort on warmer days. The camel color makes it a versatile choice for different occasions, ranging from casual outings to formal events. It pairs well with a range of outfit styles, including jeans, trousers, or a suit for business attire. The coat's timeless design and neutral color make it a worthy investment for any wardrobe.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Petite_smart_dad_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Petite_smart_dad_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Petite smart dad coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Petite smart dad coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14815.96 ms\n",
      "llama_perf_context_print: prompt eval time =   13295.34 ms /  3167 tokens (    4.20 ms per token,   238.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7793.31 ms /   178 runs   (   43.78 ms per token,    22.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   22666.25 ms /  3345 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Petite smart dad coat in black: The Black ASOS DESIGN Petite smart dad coat in the image is a stylish and versatile piece of outerwear. Crafted from a soft and comfortable fabric, the coat offers a relaxed fit that is both practical and fashionable. The material of the coat is likely to be waterproof, making it suitable for a variety of weather conditions, from rainy days to colder nights. It is best suited for casual and semi-formal occasions, serving as a chic yet functional choice for date night, outdoor events, or even business settings. Paired with jeans and sneakers, the coat exudes a laid-back vibe, while dressed up with high heels or a trench coat, it becomes an elegant and sophisticated accessory. The overall aesthetic of the coat is both modern and timeless, making it a popular choice for many seasons to come.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_overhead_rain_jacket_in_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_overhead_rain_jacket_in_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Khaki ASOS DESIGN overhead rain jacket in khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Khaki ASOS DESIGN overhead rain jacket in khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15317.55 ms\n",
      "llama_perf_context_print: prompt eval time =   13831.70 ms /  3169 tokens (    4.36 ms per token,   229.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6692.69 ms /   151 runs   (   44.32 ms per token,    22.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   22064.65 ms /  3320 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN overhead rain jacket in khaki: The Khaki ASOS DESIGN overhead rain jacket in khaki is a versatile piece of outerwear that exudes a practical and stylish appeal. The fabric is lightweight yet durable, suitable for both casual and formal wear, and ideal for rainy days. Its relaxed fit offers comfort and mobility, making it a perfect choice for outdoor activities, business attire, and even date night events. The material and texture are soft and smooth, providing a comfortable feel. Its overhead design, complete with a hood, ensures ample protection from the elements. The jacket pairs well with various bottoms, such as jeans, sneakers, high heels, and even trench coats, making it a versatile addition to any wardrobe.\n",
      "Image already exists: ./images_data/New_Look_quilted_belted_trench_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_quilted_belted_trench_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black New Look quilted belted trench coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black New Look quilted belted trench coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   16770.01 ms\n",
      "llama_perf_context_print: prompt eval time =   15224.48 ms /  3169 tokens (    4.80 ms per token,   208.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8923.12 ms /   205 runs   (   43.53 ms per token,    22.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   25750.64 ms /  3374 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look quilted belted trench coat in black: The Black New Look quilted belted trench coat in the image is a striking piece of clothing. The coat is crafted from a quilted fabric, giving it a luxurious and textured appearance. The belted design adds a touch of sophistication to the overall look. The material of the coat appears to be soft and comfortable, suggesting it's suitable for a variety of weather conditions. It is well-fitted, indicating a balance of style and comfort. The coat's design and material make it versatile, fitting well with both casual and formal wear, making it a versatile choice for various occasions, from outdoor excursions to date nights or business meetings. When paired with the right accessories, such as high heels or sneakers, the coat can transition effortlessly from a casual day out to a more polished business attire. The coat's design and the woman wearing it showcase its aesthetic appeal, making it a standout piece in any setting.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_high_shine_bomber_jacket_in_cobalt.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_high_shine_bomber_jacket_in_cobalt.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Blue ASOS DESIGN high shine bomber jacket in cobalt in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Blue ASOS DESIGN high shine bomber jacket in cobalt.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14277.06 ms\n",
      "llama_perf_context_print: prompt eval time =   12741.91 ms /  3171 tokens (    4.02 ms per token,   248.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8726.52 ms /   203 runs   (   42.99 ms per token,    23.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   23060.89 ms /  3374 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN high shine bomber jacket in cobalt: The Blue ASOS DESIGN high shine bomber jacket in cobalt is a striking piece of fashion. Its fabric has a metallic finish that gives it a high shine, making it stand out in a crowd. The style of the jacket is reminiscent of the classic bomber, with a boxy cut that falls loosely over the torso. Despite its boxy silhouette, the jacket is quite versatile, fitting comfortably and allowing for a good range of mobility. The material appears to be lightweight, suggesting that it would be suitable for casual wear in milder weather conditions. Its high shine texture would add a touch of glamour to a casual outfit or be perfect for a date night or a night out with friends. Pairing it with denim jeans and high-top sneakers would be a great way to balance its glamorous shine with a casual vibe, while a trench coat could add a sophisticated touch to a more formal setting.\n",
      "Image already exists: ./images_data/New_Look_mid_length_padded_puffer_coat_with_hood_in_dark_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_mid_length_padded_puffer_coat_with_hood_in_dark_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Green New Look mid length padded puffer coat with hood in dark khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Green New Look mid length padded puffer coat with hood in dark khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14350.51 ms\n",
      "llama_perf_context_print: prompt eval time =   12927.53 ms /  3177 tokens (    4.07 ms per token,   245.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6923.40 ms /   161 runs   (   43.00 ms per token,    23.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   21327.18 ms /  3338 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look mid length padded puffer coat with hood in dark khaki: The Green New Look mid length padded puffer coat with hood in dark khaki is a striking piece of outerwear. Its fabric, a blend of soft cotton and thick wool, exudes a sense of warmth and comfort. The style is both functional and fashionable, with a spacious hood for protection from the elements and a unique pattern that adds a touch of sophistication. The fit is relaxed yet snug, providing both style and substance. The material and texture are soft yet sturdy, making it suitable for a variety of weather conditions, from crisp winter days to mild spring evenings. Pairing it with jeans for a casual look or with a trench coat for a more formal occasion, this versatile coat is perfect for a range of events and settings.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Curve_herringbone_shacket_in_oatmeal.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Curve_herringbone_shacket_in_oatmeal.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Oatmeal ASOS DESIGN Curve herringbone shacket in oatmeal in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Oatmeal ASOS DESIGN Curve herringbone shacket in oatmeal.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14419.96 ms\n",
      "llama_perf_context_print: prompt eval time =   12966.42 ms /  3183 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6925.85 ms /   160 runs   (   43.29 ms per token,    23.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   21399.30 ms /  3343 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Curve herringbone shacket in oatmeal: The Oatmeal ASOS DESIGN Curve herringbone shacket in the image exudes a warm and inviting aesthetic. The herringbone pattern across the fabric adds a unique touch to the overall design. The shacket appears to be made of a soft cotton material, providing a comfortable fit that allows for a full range of motion. Its loose style and relaxed fit make it perfect for casual occasions or a laid-back date night. The shacket's textured design and warm color make it suitable for colder weather conditions, providing a cozy layer for rainy days or chilly evenings. Paired with high heels or sneakers, this shacket would effortlessly fit into a casual or business attire, adding a stylish touch to any outfit.\n",
      "Image already exists: ./images_data/Bershka_athletic_track_jacket_in_navy.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_athletic_track_jacket_in_navy.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Bershka athletic track jacket in navy in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Bershka athletic track jacket in navy.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14291.23 ms\n",
      "llama_perf_context_print: prompt eval time =   12873.26 ms /  3167 tokens (    4.06 ms per token,   246.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7506.70 ms /   173 runs   (   43.39 ms per token,    23.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   21854.26 ms /  3340 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka athletic track jacket in navy: The image showcases a chic navy blue Bershka athletic track jacket. The jacket is designed with a two-toned style, featuring a striking contrast of black on the sleeves and white on the front. The fabric appears to be soft and comfortable, perfect for a casual yet stylish look. The jacket is a relaxed fit, providing ample mobility for the wearer. The material seems lightweight, suggesting it's suitable for a variety of weather conditions, from cool summer days to light rain. Its versatility makes it suitable for various occasions, including casual outings, date nights, and even business attire. Paired with jeans and sneakers, it creates a sporty yet fashionable ensemble, while when worn with high heels or a trench coat, it exudes a more formal and sophisticated vibe.\n",
      "Image already exists: ./images_data/Topshop_longline_chuck_on_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_longline_chuck_on_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the black Topshop longline chuck on coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the black Topshop longline chuck on coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14276.57 ms\n",
      "llama_perf_context_print: prompt eval time =   12845.69 ms /  3163 tokens (    4.06 ms per token,   246.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7284.77 ms /   168 runs   (   43.36 ms per token,    23.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   21616.98 ms /  3331 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop longline chuck on coat in black: The black Topshop longline chuck on coat is a stylish and fashionable choice for any occasion. It boasts a smooth, soft texture, making it comfortable and ideal for any climate. The longline design adds a touch of elegance and sophistication, while the oversized fit offers a relaxed and easy-to-wear silhouette. It pairs beautifully with a variety of clothing options, from casual jeans and sneakers to formal events and date nights. The versatility of this black Topshop coat ensures that it is suitable for a wide range of weather conditions, from rainy days to cooler evenings. Whether you're looking for a statement piece for a business event or a cozy companion for a casual outing, this black Topshop longline chuck on coat in black is the perfect choice.\n",
      "Image already exists: ./images_data/Aria_Cove_vegan_leather_trench_coat_in_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Aria_Cove_vegan_leather_trench_coat_in_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the KHAKI Aria Cove vegan leather trench coat in khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the KHAKI Aria Cove vegan leather trench coat in khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14405.44 ms\n",
      "llama_perf_context_print: prompt eval time =   12945.94 ms /  3179 tokens (    4.07 ms per token,   245.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7889.55 ms /   176 runs   (   44.83 ms per token,    22.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   22353.71 ms /  3355 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Aria Cove vegan leather trench coat in khaki: The image showcases a woman in a khaki Aria Cove vegan leather trench coat, which is both stylish and comfortable. The coat features a classic double-breasted design with a belted waist, giving it a tailored look that accentuates the woman's silhouette. The fabric of the coat appears to be of high quality, providing a smooth texture that suggests both durability and comfort. The khaki color of the coat is versatile and suitable for various weather conditions, from casual outings to more formal occasions. The coat is also suitable for pairing with a variety of clothing items, such as jeans, high heels, or a business suit. Overall, the khaki Aria Cove vegan leather trench coat in khaki is a versatile and fashionable addition to any wardrobe.\n",
      "Image already exists: ./images_data/Bershka_tailored_double_breasted_coat_in_ecru.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_tailored_double_breasted_coat_in_ecru.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the ECRU Bershka tailored double breasted coat in ecru in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the ECRU Bershka tailored double breasted coat in ecru.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14537.23 ms\n",
      "llama_perf_context_print: prompt eval time =   13005.29 ms /  3175 tokens (    4.10 ms per token,   244.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5422.15 ms /   127 runs   (   42.69 ms per token,    23.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   20011.22 ms /  3302 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka tailored double breasted coat in ecru: The ECRU Bershka tailored double breasted coat in ecru, featured in the image, boasts a luxurious fabric that gives it a soft and comfortable texture. The coat's double breasted design and oversized fit suggest a relaxed and stylish aesthetic. The fabric is breathable, making it suitable for a variety of weather conditions, and it pairs well with casual jeans and sneakers, as well as formal high heels or trench coats for a sophisticated look. The coat's versatility makes it ideal for various occasions, from casual outings to formal events and date nights.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Petite_trench_coat_with_faux_leather_hood_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Petite_trench_coat_with_faux_leather_hood_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Petite trench coat with faux leather hood in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Petite trench coat with faux leather hood in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14382.90 ms\n",
      "llama_perf_context_print: prompt eval time =   12892.90 ms /  3177 tokens (    4.06 ms per token,   246.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6788.24 ms /   157 runs   (   43.24 ms per token,    23.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   21224.38 ms /  3334 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Petite trench coat with faux leather hood in black: In the image, a person is seen standing against a stark white background, wearing a black ASOS DESIGN Petite trench coat with a faux leather hood. The coat is fashionable and stylish, with a classic design that includes a belted waist, long sleeves, and a double-breasted front. The material appears to be a lightweight and breathable fabric, making it suitable for various weather conditions. The coat's design is versatile and can be worn for casual, formal, or even date night occasions. It pairs well with jeans and sneakers for a casual look or can be dressed up with high heels for a more formal style. The overall aesthetic of the coat is chic and modern, with a focus on comfort and mobility.\n",
      "Image already exists: ./images_data/Under_Armour_Training_Meridian_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Under_Armour_Training_Meridian_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Under Armour Training Meridian jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Under Armour Training Meridian jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14267.50 ms\n",
      "llama_perf_context_print: prompt eval time =   12798.62 ms /  3167 tokens (    4.04 ms per token,   247.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7655.26 ms /   176 runs   (   43.50 ms per token,    22.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   21979.98 ms /  3343 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Under Armour Training Meridian jacket in black: The BLACK Under Armour Training Meridian jacket in the image is a versatile piece of sportswear. It is made of a soft and flexible fabric, designed for comfort and ease of movement. The jacket features a high collar and long sleeves, providing excellent insulation for colder weather. Despite being a heavy-duty piece, it is lightweight, making it suitable for a variety of weather conditions. Its casual style makes it ideal for everyday wear, from casual outings to date nights. Pairing it with jeans or sneakers will give a laid-back look, while pairing it with high heels or a trench coat can elevate it to a more formal occasion. The jacket's material and texture are designed for durability and breathability, making it suitable for various activities, from running to outdoor workouts.\n",
      "Image already exists: ./images_data/Pull&Bear_oversized__bomber_jacket_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image './images_data/Pull&Bear_oversized__bomber_jacket_in_grey.jpg' -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY Pull&Bear oversized  bomber jacket in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY Pull&Bear oversized  bomber jacket in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14476.17 ms\n",
      "llama_perf_context_print: prompt eval time =   12968.65 ms /  3173 tokens (    4.09 ms per token,   244.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8550.46 ms /   198 runs   (   43.18 ms per token,    23.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   23083.18 ms /  3371 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Pull&Bear oversized  bomber jacket in grey: The grey Pull&Bear oversized bomber jacket is the focal point of the image. It's crafted from a lightweight, soft fabric that provides a comfortable fit. The oversized design gives it a relaxed and casual aesthetic, perfect for laid-back outings or chilly evenings. The jacket features a high collar, ribbed cuffs, and a front zipper that ensure it's both warm and functional. The textured material of the jacket adds to its overall appeal, making it a versatile piece that can be paired with various clothing items. It's suitable for a wide range of weather conditions, from breezy summer evenings to cooler autumn nights. Whether worn with jeans and sneakers for a casual look or paired with high heels and a trench coat for a more formal occasion, this grey Pull&Bear oversized bomber jacket is a versatile piece that effortlessly transitions from casual to formal settings.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Curve_oversized_rain_parka_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Curve_oversized_rain_parka_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Curve oversized rain parka in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Curve oversized rain parka in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14442.81 ms\n",
      "llama_perf_context_print: prompt eval time =   12963.79 ms /  3171 tokens (    4.09 ms per token,   244.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6005.34 ms /   140 runs   (   42.90 ms per token,    23.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   20500.69 ms /  3311 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Curve oversized rain parka in black: The Black ASOS DESIGN Curve oversized rain parka in the image is a standout piece of clothing. It features a loose, oversized fit, making it comfortable and easy to move around in. The fabric is smooth and seems to be waterproof, suggesting that it's well-suited for rainy days. The style is casual and fashionable, with a high neck that adds a touch of sophistication. The parka is a versatile piece that can be paired with jeans for a casual look or dressed up with high heels for a more formal occasion. Its unique design and practical functionality make it an ideal choice for various weather conditions and occasions.\n",
      "Image already exists: ./images_data/ASOS_4505_mid_layer_fleece_back_jacket_with_reflective_piping.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_4505_mid_layer_fleece_back_jacket_with_reflective_piping.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Dark Teal ASOS 4505 mid layer fleece back jacket with reflective piping in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Dark Teal ASOS 4505 mid layer fleece back jacket with reflective piping.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14402.84 ms\n",
      "llama_perf_context_print: prompt eval time =   12904.04 ms /  3187 tokens (    4.05 ms per token,   246.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8875.32 ms /   206 runs   (   43.08 ms per token,    23.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   23335.21 ms /  3393 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS 4505 mid layer fleece back jacket with reflective piping: The Dark Teal ASOS 4505 mid layer fleece back jacket with reflective piping is a versatile piece of clothing that offers a stylish and functional look. The jacket features a full-length zipper, which is designed to provide both style and functionality. The fabric of the jacket is soft and comfortable to touch, making it ideal for casual wear. Its loose fit ensures ease of movement, making it suitable for various activities. The reflective piping adds a touch of safety and style, enhancing the jacket's overall aesthetic. This jacket would be best suited for casual wear, such as jeans and sneakers, or for more formal occasions when paired with a trench coat. Its lightweight and breathable design makes it perfect for a variety of weather conditions, including mild summer days and chilly autumn nights. The jacket's stylish design and comfortable fit make it a great choice for anyone looking for a versatile and stylish mid layer fleece jacket.\n",
      "Image already exists: ./images_data/Topshop_double_breasted_coat_in_green.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_double_breasted_coat_in_green.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREEN Topshop double breasted coat in green in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREEN Topshop double breasted coat in green.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14301.13 ms\n",
      "llama_perf_context_print: prompt eval time =   12814.22 ms /  3163 tokens (    4.05 ms per token,   246.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8669.55 ms /   200 runs   (   43.35 ms per token,    23.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   23027.68 ms /  3363 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop double breasted coat in green: The green Topshop double breasted coat in the image is a vibrant and eye-catching piece of outerwear. Its fabric appears to be a luxurious cotton, giving it a soft and comfortable texture. The style is elegant and chic, with a double breasted design that adds a touch of sophistication. The coat's green color is both bold and versatile, making it suitable for a variety of occasions. It would be perfect for casual events, yet it also has the refinement to be worn for more formal occasions. The coat's design and material make it ideal for colder weather conditions, providing warmth and protection. It can be paired with a variety of outfits, from jeans and sneakers for a casual look to high heels and trench coats for a more formal and polished appearance. The coat's overall aesthetic is a blend of style, comfort, and practicality, making it a standout piece in any wardrobe.\n",
      "Image already exists: ./images_data/Topshop_Sno_borg_zip_through_jacket_in_green.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Sno_borg_zip_through_jacket_in_green.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREEN Topshop Sno borg zip through jacket in green in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREEN Topshop Sno borg zip through jacket in green.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14387.96 ms\n",
      "llama_perf_context_print: prompt eval time =   12889.15 ms /  3169 tokens (    4.07 ms per token,   245.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6549.59 ms /   152 runs   (   43.09 ms per token,    23.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   20990.72 ms /  3321 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Sno borg zip through jacket in green: The green Topshop Sno borg zip through jacket in the image features a soft, fuzzy texture that gives it a cozy, warm appearance. Its fabric is designed to provide comfort and mobility, making it suitable for a variety of casual occasions. The zipper detail adds a stylish touch to the overall aesthetic of the jacket. The fit of the jacket appears to be relaxed, offering a comfortable and versatile wear. As for material, it seems to be a blend of cotton and wool, which is both breathable and suitable for cooler weather. Pairing it with denim jeans or sneakers could complete a casual look, while adding a trench coat could elevate it for a more formal or chic occasion.\n",
      "Image already exists: ./images_data/New_Look_belted_gilet_with_tie_waist_detail_in_light_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_belted_gilet_with_tie_waist_detail_in_light_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Light green New Look belted gilet with tie waist detail in light khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Light green New Look belted gilet with tie waist detail in light khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14389.99 ms\n",
      "llama_perf_context_print: prompt eval time =   12942.21 ms /  3179 tokens (    4.07 ms per token,   245.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6282.04 ms /   145 runs   (   43.32 ms per token,    23.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   20725.47 ms /  3324 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look belted gilet with tie waist detail in light khaki: The light green New Look belted gilet, characterized by its tie waist detail in light khaki, is a versatile piece that stands out in this image. It's made of a soft fabric that gives it a comfortable and relaxed fit. The lightweight material suggests that it's suitable for various weather conditions, from casual, outdoor wear to business attire. Pairing it with blue jeans and black sneakers creates a stylish look for a casual day, while pairing it with high heels and a trench coat could elevate it for a formal or date night occasion. Its overall aesthetic is a blend of casual and formal, making it a versatile addition to any wardrobe.\n",
      "Image already exists: ./images_data/New_Look_Curve_quilted_puffer_coat_in_dark_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_Curve_quilted_puffer_coat_in_dark_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Khaki New Look Curve quilted puffer coat in dark khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Khaki New Look Curve quilted puffer coat in dark khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14363.57 ms\n",
      "llama_perf_context_print: prompt eval time =   12895.85 ms /  3177 tokens (    4.06 ms per token,   246.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6329.22 ms /   148 runs   (   42.76 ms per token,    23.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   20744.56 ms /  3325 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look Curve quilted puffer coat in dark khaki: The Khaki New Look Curve quilted puffer coat in dark khaki is a stylish and fashionable piece of outerwear. Its fabric has a soft and comfortable texture, making it suitable for cooler days. The quilting on the coat adds a touch of warmth and style, while the dark khaki color gives it a modern and contemporary look. Its fit is relaxed, providing ample room for movement, and it's suitable for casual to semi-formal occasions. Paired with white pants or jeans, it can easily transition from a casual day out to a more sophisticated date night. The coat's design and material make it an ideal choice for both daytime and evening wear.\n",
      "Image already exists: ./images_data/Topshop_Tall_sleeveless_puffer_gilet_jacket_in_off_white.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Tall_sleeveless_puffer_gilet_jacket_in_off_white.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the off white Topshop Tall sleeveless puffer gilet jacket in off white in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the off white Topshop Tall sleeveless puffer gilet jacket in off white.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14689.66 ms\n",
      "llama_perf_context_print: prompt eval time =   13228.85 ms /  3177 tokens (    4.16 ms per token,   240.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7984.14 ms /   178 runs   (   44.85 ms per token,    22.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   22737.60 ms /  3355 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Tall sleeveless puffer gilet jacket in off white: The off-white Topshop Tall sleeveless puffer gilet jacket in the image is a versatile piece that stands out due to its unique design and fabric. The jacket features a tall collar and large pockets on the front, adding a touch of style to its overall aesthetic. The fabric appears to be soft and comfortable, providing a cozy feel that is perfect for cooler days. Despite its puffy nature, it doesn't seem to be too bulky, suggesting good mobility for the wearer. The material is likely a blend of cotton and wool, providing a good balance between warmth and breathability. This jacket would be ideal for casual outings or even for a chic date night look. It would pair well with denim jeans and sneakers for a laid-back look or with high heels and a trench coat for a more formal occasion.\n",
      "Image already exists: ./images_data/Forever_New_formal_wrap_coat_with_tie_belt_in_camel.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Forever_New_formal_wrap_coat_with_tie_belt_in_camel.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Camel Forever New formal wrap coat with tie belt in camel in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Camel Forever New formal wrap coat with tie belt in camel.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14946.44 ms\n",
      "llama_perf_context_print: prompt eval time =   13419.09 ms /  3171 tokens (    4.23 ms per token,   236.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6911.89 ms /   155 runs   (   44.59 ms per token,    22.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   21924.06 ms /  3326 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Forever New formal wrap coat with tie belt in camel: In the image, the Camel Forever New formal wrap coat with tie belt in camel is the central focus. The coat is made of a soft, lightweight fabric that gives it a luxurious and comfortable feel. Its long length and tie belt cinch the waist, creating a flattering silhouette. The coat's open front and wide lapels add a touch of sophistication to its design. The material of the coat suggests that it is breathable and suitable for a variety of weather conditions. It pairs well with both casual and formal attire, making it versatile for different occasions. Whether worn with jeans and sneakers or dressed up with high heels and a trench coat, this coat adds an elegant touch to any outfit.\n",
      "Image already exists: ./images_data/Topshop_oversized_collar_corduroy_bomber_jacket_in_beige.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_oversized_collar_corduroy_bomber_jacket_in_beige.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BEIGE Topshop oversized collar corduroy bomber jacket in beige in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BEIGE Topshop oversized collar corduroy bomber jacket in beige.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15205.47 ms\n",
      "llama_perf_context_print: prompt eval time =   13699.39 ms /  3177 tokens (    4.31 ms per token,   231.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8123.03 ms /   189 runs   (   42.98 ms per token,    23.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   23384.79 ms /  3366 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop oversized collar corduroy bomber jacket in beige: The BEIGE Topshop oversized collar corduroy bomber jacket in the image is a versatile piece of clothing. Crafted from corduroy, a fabric known for its durability and texture, the jacket exudes a sense of comfort and warmth. The oversized collar adds a touch of style, making it a perfect choice for those who enjoy a relaxed fit. The top part of the jacket is loose, allowing for ample mobility, while the bottom part fits snugly, providing a sense of security. This light beige color is suitable for various weather conditions, making it a versatile piece for both casual and formal occasions. It pairs well with a range of bottoms, from jeans and sneakers to dress pants and high heels. The jacket's casual and laid-back aesthetic makes it an ideal choice for relaxed settings, yet its stylish elements lend it to more formal occasions as well.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Tall_cotton_pocket_shacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Tall_cotton_pocket_shacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Tall cotton pocket shacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Tall cotton pocket shacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14449.66 ms\n",
      "llama_perf_context_print: prompt eval time =   12983.87 ms /  3167 tokens (    4.10 ms per token,   243.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7351.31 ms /   160 runs   (   45.95 ms per token,    21.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   21863.29 ms /  3327 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Tall cotton pocket shacket in black: The black ASOS DESIGN Tall cotton pocket shacket in this image is a versatile piece of outerwear. It features a relaxed fit that provides both comfort and mobility. The fabric is soft and has a subtle texture, making it suitable for various weather conditions. It's breathable for warmer days and can also be worn in cooler weather due to its cotton material. The shacket style is perfect for casual wear, yet it's also stylish enough to be worn to semi-formal events. It pairs well with jeans for a laid-back look or with high heels and a trench coat for a more sophisticated style. Overall, the black ASOS DESIGN Tall cotton pocket shacket is a versatile and fashionable addition to any wardrobe.\n",
      "Image already exists: ./images_data/Vero_Moda_belted_wrap_coat_in_beige.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Vero_Moda_belted_wrap_coat_in_beige.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BEIGE Vero Moda belted wrap coat in beige in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BEIGE Vero Moda belted wrap coat in beige.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14401.04 ms\n",
      "llama_perf_context_print: prompt eval time =   12909.40 ms /  3171 tokens (    4.07 ms per token,   245.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6853.59 ms /   160 runs   (   42.83 ms per token,    23.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   21308.54 ms /  3331 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Vero Moda belted wrap coat in beige: The beige Vero Moda belted wrap coat in the image exudes a luxurious and stylish vibe. It features a belted wrap design, adding a touch of elegance and sophistication. The coat is made of a soft and lightweight fabric, providing a comfortable fit and excellent mobility. The material appears to be of high quality, suggesting that it is suitable for various weather conditions, including milder days or slightly cooler evenings. It pairs well with both casual and formal attire, making it an ideal choice for a variety of occasions such as date night, business meetings, or even outdoor events. The coat's versatile design allows it to effortlessly transition from day to night, making it a must-have piece in any wardrobe.\n",
      "Image already exists: ./images_data/In_The_Style_utility_pocket_detail_belted_jacket_in_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/In_The_Style_utility_pocket_detail_belted_jacket_in_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLUE In The Style utility pocket detail belted jacket in blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLUE In The Style utility pocket detail belted jacket in blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14209.78 ms\n",
      "llama_perf_context_print: prompt eval time =   12816.18 ms /  3169 tokens (    4.04 ms per token,   247.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5319.71 ms /   116 runs   (   45.86 ms per token,    21.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   19590.00 ms /  3285 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for In The Style utility pocket detail belted jacket in blue: This image features a woman dressed in a chic, blue utility pocket detail belted jacket paired with a beige skirt. The jacket exudes a casual and stylish vibe, characterized by its loose fit and comfortable feel. Made of a soft fabric, it appears to be perfect for a variety of weather conditions, including warmer ones. Its overall aesthetic suggests it would be well-suited for a range of occasions, from casual outings to semi-formal events, and would pair well with various bottoms, from jeans to high heels.\n",
      "Image already exists: ./images_data/Noisy_May_cord_oversized_shacket_in_brown.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Noisy_May_cord_oversized_shacket_in_brown.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN Noisy May cord oversized shacket in brown in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN Noisy May cord oversized shacket in brown.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14905.30 ms\n",
      "llama_perf_context_print: prompt eval time =   13378.45 ms /  3169 tokens (    4.22 ms per token,   236.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5695.46 ms /   132 runs   (   43.15 ms per token,    23.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   20653.17 ms /  3301 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Noisy May cord oversized shacket in brown: The image features a person wearing a brown Noisy May cord oversized shacket. The shacket is characterized by its loose fit and is designed with a casual style. It displays a unique pattern of corduroy, which gives it a textured and warm appearance. The shacket's material appears soft and comfortable, offering a high level of mobility to the wearer. This shacket is suitable for various weather conditions, particularly when an additional layer is desired. It is best suited for casual outings, date nights, and outdoor wear. It pairs well with both jeans and trench coats, providing a versatile look for different occasions.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_leather_ultimate_biker_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_leather_ultimate_biker_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN leather ultimate biker jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN leather ultimate biker jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14282.44 ms\n",
      "llama_perf_context_print: prompt eval time =   12786.76 ms /  3167 tokens (    4.04 ms per token,   247.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6711.88 ms /   155 runs   (   43.30 ms per token,    23.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   21049.00 ms /  3322 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN leather ultimate biker jacket in black: The Black ASOS DESIGN leather ultimate biker jacket in the image exudes a chic, edgy style with its high-quality black leather material. The jacket is fitted, providing a stylish silhouette, yet it appears to be comfortable and allows for a good range of motion. The overall aesthetic is bold and fashionable, suitable for various occasions from casual outings to stylish date nights. Paired with blue jeans, black boots, and a pair of black sunglasses, the jacket would complete a trendy and sophisticated look. The material of the jacket is thick and sturdy, making it perfect for colder weather conditions, while its fitted design can still be worn on warmer days if paired with a light shirt underneath.\n",
      "Image already exists: ./images_data/Gym_King_utility_gilet_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Gym_King_utility_gilet_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Gym King utility gilet in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Gym King utility gilet in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14259.52 ms\n",
      "llama_perf_context_print: prompt eval time =   12772.35 ms /  3163 tokens (    4.04 ms per token,   247.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6475.22 ms /   150 runs   (   43.17 ms per token,    23.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   20787.69 ms /  3313 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Gym King utility gilet in black: The black Gym King utility gilet in this image is made of a soft and stretchy fabric. Its style is practical and functional, with a zipper down the front for easy closure and multiple pockets for storage. The gilet's pattern is simple and uncluttered, consisting of a solid black color, and it features a high collar for extra warmth. It fits loosely, providing a comfortable fit without restricting movement. The gilet is made of a lightweight and breathable material, making it suitable for cool weather conditions. It is ideal for casual and outdoor wear, and it pairs well with jeans and sneakers for a relaxed look, or with a trench coat for a more formal occasion.\n",
      "Image already exists: ./images_data/River_Island_faux_leather_oversized_biker_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/River_Island_faux_leather_oversized_biker_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK River Island faux leather oversized biker jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK River Island faux leather oversized biker jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14189.17 ms\n",
      "llama_perf_context_print: prompt eval time =   12772.56 ms /  3171 tokens (    4.03 ms per token,   248.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7186.68 ms /   168 runs   (   42.78 ms per token,    23.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   21431.08 ms /  3339 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for River Island faux leather oversized biker jacket in black: The black River Island faux leather oversized biker jacket in the image is made of a material that is both soft and smooth, giving it a luxurious feel. The jacket is characterized by its loose fit and comfortable texture, making it suitable for both casual and formal occasions. Despite its oversized design, it's not overly bulky and allows for a good range of motion. Its style is reminiscent of a biker aesthetic, with the faux leather material and the oversized silhouette. The jacket's overall aesthetic is edgy and chic, yet it's not too heavy or rigid, making it suitable for a variety of weather conditions. It pairs well with denim jeans for a casual look, or with high heels and a trench coat for a more formal or sophisticated outfit.\n",
      "Image already exists: ./images_data/Tommy_Jeans_oversized_hooded_denim_jacket_in_multi.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Tommy_Jeans_oversized_hooded_denim_jacket_in_multi.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Denim Dark Tommy Jeans oversized hooded denim jacket in multi in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Denim Dark Tommy Jeans oversized hooded denim jacket in multi.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14089.39 ms\n",
      "llama_perf_context_print: prompt eval time =   12674.94 ms /  3173 tokens (    3.99 ms per token,   250.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8782.52 ms /   205 runs   (   42.84 ms per token,    23.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   22931.44 ms /  3378 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Tommy Jeans oversized hooded denim jacket in multi: In the image, we see a model showcasing a Denim Dark Tommy Jeans oversized hooded denim jacket. The jacket is made of a heavy-weight denim material, giving it a sturdy and durable feel. The color scheme is multi, with the top half in a lighter shade of blue and the bottom half in a darker shade, adding a unique touch to the classic denim style. The jacket features a hood for extra warmth and protection, and it's designed with a drawstring, providing a snug fit. The overall aesthetic of the jacket is casual and chic, making it suitable for various weather conditions and occasions. Pair it with skinny jeans and sneakers for a cool, casual look, or dress it up with a trench coat and high heels for a more formal and elegant appearance. The jacket's comfortable and breathable design ensures both comfort and mobility, making it an excellent choice for a variety of outfits and lifestyles.\n",
      "Image already exists: ./images_data/Topshop_textured_short_coat_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_textured_short_coat_in_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY MARL Topshop textured short coat in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY MARL Topshop textured short coat in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14339.03 ms\n",
      "llama_perf_context_print: prompt eval time =   12878.13 ms /  3171 tokens (    4.06 ms per token,   246.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6978.78 ms /   163 runs   (   42.81 ms per token,    23.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   21372.10 ms /  3334 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop textured short coat in grey: The GREY MARL Topshop textured short coat in grey is a chic and stylish addition to any wardrobe. Crafted from a soft fabric, it boasts a textured pattern that adds depth and interest to the overall aesthetic. The loose fit of the coat ensures a comfortable wear, allowing for a range of movement. The material seems to be lightweight and breathable, making it an ideal choice for cooler weather conditions. Pairing it with a pair of jeans and sneakers creates a casual, yet fashionable look. For a more formal occasion, it can be paired with high heels or a trench coat. The versatility of this coat makes it suitable for a variety of outfits, from date nights to outdoor wear and business attire.\n",
      "Image already exists: ./images_data/NA-KD_x_Moa_Mattsson_belted_trench_coat_with_faux_fur_in_sage.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/NA-KD_x_Moa_Mattsson_belted_trench_coat_with_faux_fur_in_sage.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the SAGE NA-KD x Moa Mattsson belted trench coat with faux fur in sage in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the SAGE NA-KD x Moa Mattsson belted trench coat with faux fur in sage.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14959.06 ms\n",
      "llama_perf_context_print: prompt eval time =   13542.73 ms /  3187 tokens (    4.25 ms per token,   235.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7162.26 ms /   158 runs   (   45.33 ms per token,    22.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   22185.05 ms /  3345 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for NA-KD x Moa Mattsson belted trench coat with faux fur in sage: In the image, a woman is seen wearing a SAGE NA-KD x Moa Mattsson belted trench coat with faux fur in sage. The coat is characterized by its long length, wide lapels, and a belted waist. The fabric of the coat appears to be a blend of materials, offering a soft texture that suggests comfort and mobility. Its style, with its wide lapels and belted waist, lends a sense of formality, making it ideal for colder weather conditions. The coat would be best suited for occasions such as formal events, business attire, or even date nights. It pairs well with a variety of bottoms, from jeans to high heels, and can be easily dressed up or down depending on the occasion.\n",
      "Image already exists: ./images_data/Topshop_borg_trench_coat_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_borg_trench_coat_in_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY Topshop borg trench coat in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY Topshop borg trench coat in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14828.74 ms\n",
      "llama_perf_context_print: prompt eval time =   13382.84 ms /  3167 tokens (    4.23 ms per token,   236.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8174.71 ms /   179 runs   (   45.67 ms per token,    21.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   23061.53 ms /  3346 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop borg trench coat in grey: The grey Topshop borg trench coat in the image is a versatile piece that exudes a casual yet stylish vibe. Its fabric appears soft and light, making it both comfortable and breathable. The coat's style is characterized by its long, relaxed fit and its high collar, which gives it an elegant and fashionable look. The borg texture adds a touch of coziness and warmth to the overall aesthetic. Its loose fit and high collar make it suitable for various weather conditions, from cooler days to milder ones. Pairing it with jeans or sneakers would give it a casual look, while high heels or trench coats could elevate it for more formal occasions. Its versatility and versatile style make it an ideal choice for various occasions, from casual to formal, date night to outdoor wear, and business attire.\n",
      "Image already exists: ./images_data/Whistles_quilted_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Whistles_quilted_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Whistles quilted jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Whistles quilted jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15287.07 ms\n",
      "llama_perf_context_print: prompt eval time =   13831.52 ms /  3165 tokens (    4.37 ms per token,   228.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6473.02 ms /   146 runs   (   44.34 ms per token,    22.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   21813.80 ms /  3311 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Whistles quilted jacket in black: The image showcases a sleek, black Whistles quilted jacket, exuding a modern, stylish appeal. Its quilted design and diamond pattern are evident, with the material appearing to be a soft, comfortable fabric. The jacket is fitted yet relaxed, allowing for ease of movement. The overall aesthetic is versatile and can be paired well with a variety of clothing items, ranging from casual jeans and sneakers to more formal high heels and a trench coat. Its quilted design suggests it's suitable for cooler weather conditions, providing warmth while maintaining a sophisticated look. The black color offers a classic, timeless appeal, making it a versatile addition to any wardrobe.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Petite_rubberised_rain_parka_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Petite_rubberised_rain_parka_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Petite rubberised rain parka coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Petite rubberised rain parka coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14344.33 ms\n",
      "llama_perf_context_print: prompt eval time =   12895.27 ms /  3173 tokens (    4.06 ms per token,   246.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8401.17 ms /   196 runs   (   42.86 ms per token,    23.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   22801.56 ms /  3369 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Petite rubberised rain parka coat in black: The black ASOS DESIGN Petite rubberised rain parka coat in the image is a fashionable and functional piece of outerwear. Its fabric appears to be a blend of rubber and possibly cotton or nylon, giving it a sturdy and waterproof texture. The style of the coat is modern and casual, with a high collar and hood, ensuring protection from the elements. The fit of the coat is loose, providing a comfortable and roomy feel, while maintaining a stylish and fashion-forward aesthetic. The overall aesthetic of the coat is versatile, suitable for a variety of weather conditions, from rainy days to mildly cool temperatures. It is best suited for casual occasions, pairing well with jeans, sneakers, and high heels, or even a trench coat for a more formal look. The coat's design and functionality make it an ideal choice for outdoor wear, business attire, or for a stylish date night look.\n",
      "Image already exists: ./images_data/Pull&Bear_puffer_coat_with_hood_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image './images_data/Pull&Bear_puffer_coat_with_hood_in_grey.jpg' -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY Pull&Bear puffer coat with hood in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY Pull&Bear puffer coat with hood in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14206.09 ms\n",
      "llama_perf_context_print: prompt eval time =   12808.12 ms /  3171 tokens (    4.04 ms per token,   247.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9851.10 ms /   225 runs   (   43.78 ms per token,    22.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   24118.24 ms /  3396 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Pull&Bear puffer coat with hood in grey: The grey Pull&Bear puffer coat with a hood in the image has a fluffy and cozy texture, making it perfect for chilly weather. The style is casual yet fashionable, with a modern design that includes a high collar, zip-up front, and spacious pockets. It is a loose-fitting garment that allows for freedom of movement, and the comfort level is high due to the insulation material. The material appears to be soft and warm, suitable for both rainy and snowy days, providing ample protection against the elements.\n",
      " \n",
      " For casual occasions, the coat is best suited, as it pairs well with a variety of outfits. It can be worn with jeans and sneakers for a laid-back look or paired with high heels and a trench coat for a more formal appearance. The versatility of the coat allows it to be worn in different settings, from casual outings to business meetings. Overall, the grey Pull&Bear puffer coat with a hood is a practical and stylish choice for various occasions and weather conditions.\n",
      "Image already exists: ./images_data/The_Frolic_tweed_boucle_cropped_jacket_co-ord_in_multi.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/The_Frolic_tweed_boucle_cropped_jacket_co-ord_in_multi.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the MULTI TWEED The Frolic tweed boucle cropped jacket co-ord in multi in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the MULTI TWEED The Frolic tweed boucle cropped jacket co-ord in multi.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15592.10 ms\n",
      "llama_perf_context_print: prompt eval time =   14072.92 ms /  3185 tokens (    4.42 ms per token,   226.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8873.06 ms /   193 runs   (   45.97 ms per token,    21.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   24531.83 ms /  3378 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for The Frolic tweed boucle cropped jacket co-ord in multi: The MULTI TWEED The Frolic tweed boucle cropped jacket co-ord in multi is a stylish piece of clothing that is both comfortable and versatile. The fabric is a mix of different textures and patterns, creating a unique and eye-catching aesthetic. The cropped length and fitted silhouette give it a modern and chic look, making it suitable for a variety of occasions from casual outings to formal events. The jacket's material is soft and comfortable, allowing for ease of movement. Pairing it with jeans or sneakers for a casual look, or high heels for a more formal occasion, the MULTI TWEED The Frolic tweed boucle cropped jacket co-ord in multi is a great addition to any wardrobe. It's suitable for various weather conditions due to its breathable material, making it a perfect choice for outdoor wear or even business attire.\n",
      "Image already exists: ./images_data/Topshop_Tall_faux_leather_shearling_aviator_biker_jacket_in_ecru.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Tall_faux_leather_shearling_aviator_biker_jacket_in_ecru.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Ecru Topshop Tall faux leather shearling aviator biker jacket in ecru in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Ecru Topshop Tall faux leather shearling aviator biker jacket in ecru.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15266.43 ms\n",
      "llama_perf_context_print: prompt eval time =   13780.55 ms /  3183 tokens (    4.33 ms per token,   230.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9100.62 ms /   209 runs   (   43.54 ms per token,    22.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   24433.87 ms /  3392 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Tall faux leather shearling aviator biker jacket in ecru: This image features a young woman dressed in a beige, faux leather jacket from Topshop. The jacket is characterized by its shearling aviator style and is designed with a combination of leather and suede. It features a high collar, adding a touch of elegance and warmth to the piece. The fit of the jacket is loose, providing ample room for layering. Its comfort level is high due to the soft fabric and the added texture of the faux leather. Despite its thickness, the jacket does not seem to hinder mobility, allowing the wearer to move freely. The light beige color and the aviator style make it suitable for a variety of weather conditions, ranging from casual to formal occasions, from date nights to outdoor activities. It pairs well with jeans and sneakers, while a trench coat or high heels could elevate the look for more formal events. The overall aesthetic of the jacket exudes a sense of sophistication, making it a versatile addition to any wardrobe.\n",
      "Image already exists: ./images_data/Topshop_mid_length_tie_belted_puffer_jacket_in_forest_green.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_mid_length_tie_belted_puffer_jacket_in_forest_green.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the FOREST GREEN Topshop mid length tie belted puffer jacket in forest green in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the FOREST GREEN Topshop mid length tie belted puffer jacket in forest green.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15008.32 ms\n",
      "llama_perf_context_print: prompt eval time =   13453.49 ms /  3179 tokens (    4.23 ms per token,   236.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9119.11 ms /   211 runs   (   43.22 ms per token,    23.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   24184.24 ms /  3390 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop mid length tie belted puffer jacket in forest green: The Forest Green Topshop mid length tie belted puffer jacket in forest green in the image is a standout piece of clothing. It is made of a thick, puffy fabric, which gives it an ample and warm appearance, perfect for colder weather or to add an extra layer for protection against the elements. The material of the jacket appears to be soft and comfortable, allowing for a good level of mobility. The design and style of the jacket are casual yet stylish, with a distinctive mid length cut that hits around the hip, creating a modern and edgy look. The jacket is belted with a tie, giving it a distinctive and trendy touch. Overall, the jacket is versatile and can be worn for a variety of occasions, ranging from casual outings to more formal events. Paired with jeans and sneakers, it adds an extra layer of style to a casual outfit. For more formal occasions, it can be teamed with high heels and a trench coat, creating a polished and sophisticated look.\n",
      "Image already exists: ./images_data/Stradivarius_STR_faux_leather_fur_trim_coat_in_brown.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_STR_faux_leather_fur_trim_coat_in_brown.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN Stradivarius STR faux leather fur trim coat in brown in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN Stradivarius STR faux leather fur trim coat in brown.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14773.13 ms\n",
      "llama_perf_context_print: prompt eval time =   13188.03 ms /  3175 tokens (    4.15 ms per token,   240.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6668.38 ms /   156 runs   (   42.75 ms per token,    23.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   21496.74 ms /  3331 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius STR faux leather fur trim coat in brown: The BROWN Stradivarius STR faux leather fur trim coat in the image presents a luxurious and fashionable ensemble. The coat is crafted from a soft, plush fabric that gives it a snug and comfortable fit. Its faux leather material and texture lend an elegant and sophisticated feel, while the fur trim adds a touch of warmth and coziness. Its loose and relaxed fit allows for ease of movement and breathability, making it suitable for various weather conditions, from chilly evenings to casual, outdoor outings. Paired with denim jeans and high heels, it's perfect for a date night or a night out with friends. Its versatile style and aesthetic make it an ideal choice for both casual and formal occasions.\n",
      "Image already exists: ./images_data/ASOS_EDITION_hooded_trench_coat_in_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_EDITION_hooded_trench_coat_in_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Khaki ASOS EDITION hooded trench coat in khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Khaki ASOS EDITION hooded trench coat in khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15729.03 ms\n",
      "llama_perf_context_print: prompt eval time =   14257.31 ms /  3173 tokens (    4.49 ms per token,   222.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6860.42 ms /   135 runs   (   50.82 ms per token,    19.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   22662.00 ms /  3308 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS EDITION hooded trench coat in khaki: The khaki ASOS EDITION hooded trench coat in the image exudes a casual and stylish aesthetic. The fabric appears to be a lightweight cotton or a cotton blend, giving it a soft and comfortable texture. The style of the trench coat is relaxed with a roomy fit, which allows for good mobility. The hooded design provides additional warmth and protection, making it suitable for a range of weather conditions, including light rain. The coat would be best suited for casual occasions and outdoor wear, pairing well with both jeans and high heels. Its versatility and practicality make it a versatile piece for various seasons and settings.\n",
      "Image already exists: ./images_data/Miss_Selfridge_Petite_cropped_denim_jacket_in_midwash_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Miss_Selfridge_Petite_cropped_denim_jacket_in_midwash_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Midwash Blue Miss Selfridge Petite cropped denim jacket in midwash blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Midwash Blue Miss Selfridge Petite cropped denim jacket in midwash blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15500.93 ms\n",
      "llama_perf_context_print: prompt eval time =   14051.53 ms /  3181 tokens (    4.42 ms per token,   226.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9932.31 ms /   224 runs   (   44.34 ms per token,    22.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   25492.83 ms /  3405 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Miss Selfridge Petite cropped denim jacket in midwash blue: The Midwash Blue Miss Selfridge Petite cropped denim jacket in midwash blue is a fashionable and stylish piece of clothing. Its fabric is made of soft and comfortable denim material, which is known for its durability and style. The jacket is cropped, giving it a trendy and contemporary look. It features three gold-toned buttons that add a touch of elegance to the overall design. The cropped fit of the jacket provides a relaxed and casual feel, while also maintaining a sense of style. The material and texture of the jacket are suitable for various weather conditions, offering breathability for warmer days and a sense of warmth for colder seasons. The jacket is best suited for casual occasions, such as a day out with friends or a casual date night. It pairs well with both jeans and sneakers, adding an element of style to any outfit. Overall, the Midwash Blue Miss Selfridge Petite cropped denim jacket in midwash blue is a versatile and fashionable garment that can easily complement various styles and occasions.\n",
      "Image already exists: ./images_data/River_Island_herringbone_duster_coat_in_dark_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/River_Island_herringbone_duster_coat_in_dark_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the GREY - DARK River Island herringbone duster coat in dark grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the GREY - DARK River Island herringbone duster coat in dark grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14477.56 ms\n",
      "llama_perf_context_print: prompt eval time =   12969.89 ms /  3177 tokens (    4.08 ms per token,   244.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6596.55 ms /   153 runs   (   43.11 ms per token,    23.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   21128.20 ms /  3330 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for River Island herringbone duster coat in dark grey: The River Island herringbone duster coat in dark grey is a stylish piece that exudes a casual yet chic vibe. Its fabric appears to be soft and lightweight, suggesting a comfortable fit. The coat's loose style offers great mobility, making it ideal for various occasions. The herringbone pattern on the coat adds a touch of sophistication to the overall aesthetic. The coat pairs well with both casual and formal attire, from jeans and sneakers to high heels and trench coats. Its lightweight material suggests that it's suitable for a variety of weather conditions, from cool autumn days to breezy summer evenings. Overall, this duster coat is a versatile and fashionable addition to any wardrobe.\n",
      "Image already exists: ./images_data/Stradivarius_padded_puffer_jacket_in_ecru.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_padded_puffer_jacket_in_ecru.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the ECRU Stradivarius padded puffer jacket in ecru in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the ECRU Stradivarius padded puffer jacket in ecru.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15729.47 ms\n",
      "llama_perf_context_print: prompt eval time =   14268.07 ms /  3173 tokens (    4.50 ms per token,   222.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7841.53 ms /   183 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   23625.81 ms /  3356 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius padded puffer jacket in ecru: The ECRU Stradivarius padded puffer jacket in ecru is a fashionable piece of outerwear. The fabric of the jacket has a soft, plush texture that gives it a luxurious feel. The style is characterized by the puffiness of the padded sleeves and body, creating a voluminous appearance that is both stylish and warm. The ecru color adds a subtle sheen to the material, making it versatile and suitable for various occasions. Its loose fit offers comfort and mobility, allowing for ease of movement. Pairing it with jeans, sneakers, or high heels would complement the jacket's aesthetic, while the padded design also makes it suitable for cooler weather conditions. Its overall aesthetic exudes a sense of sophistication, making it an ideal choice for formal events, date nights, and outdoor wear during the colder months.\n",
      "Image already exists: ./images_data/Weekday_Daphne_double_breasted_formal_maxi_coat_in_dark_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Weekday_Daphne_double_breasted_formal_maxi_coat_in_dark_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the HERRINGBONE Weekday Daphne double breasted formal maxi coat in dark grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the HERRINGBONE Weekday Daphne double breasted formal maxi coat in dark grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14460.00 ms\n",
      "llama_perf_context_print: prompt eval time =   12942.23 ms /  3183 tokens (    4.07 ms per token,   245.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6293.47 ms /   145 runs   (   43.40 ms per token,    23.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   20806.50 ms /  3328 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Weekday Daphne double breasted formal maxi coat in dark grey: The HERRINGBONE Weekday Daphne double breasted formal maxi coat in dark grey presents a stylish and sophisticated choice. The coat is made of a soft, lightweight fabric, offering both comfort and a sense of luxury. Its loose fit allows for plenty of mobility and ease of movement, making it suitable for various weather conditions. The coat's double breasted design and the intricate herringbone pattern add to its elegant aesthetic. Paired with black pants, this coat is best suited for formal occasions or a classic, timeless date night outfit. The versatile design also allows it to pair well with various footwear options, such as heels, sneakers, or dress shoes.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Petite_cropped_rain_jacket_with_hood_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Petite_cropped_rain_jacket_with_hood_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black ASOS DESIGN Petite cropped rain jacket with hood in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black ASOS DESIGN Petite cropped rain jacket with hood in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14432.44 ms\n",
      "llama_perf_context_print: prompt eval time =   12925.80 ms /  3173 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10330.24 ms /   241 runs   (   42.86 ms per token,    23.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   24820.97 ms /  3414 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Petite cropped rain jacket with hood in black: The black ASOS DESIGN Petite cropped rain jacket with hood is a stylish and functional piece of outerwear. Its fabric appears to be a smooth and waterproof material, perfect for rainy days. The style is modern and fashionable, featuring a cropped design and a hood for added protection. The fit of the jacket seems to be slim, providing a sleek and contemporary look. The comfort level is likely to be good due to the soft texture of the material, which is likely to be lightweight and breathable. In terms of mobility, the jacket's design allows for ease of movement. As for suitable weather conditions, this jacket would be ideal for rainy days, offering protection from the elements while still allowing for some breathability. For occasions, it is versatile and can be worn casually, for date nights, or even in business attire when paired with dress pants and a dress shirt. For footwear, it pairs well with sneakers, boots, or even high heels for a more formal look. Overall, this black ASOS DESIGN Petite cropped rain jacket with hood is a chic and practical addition to any wardrobe.\n",
      "Image already exists: ./images_data/Only_longline_formal_coat_in_chocolate.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Only_longline_formal_coat_in_chocolate.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the HOT FUDGE Only longline formal coat in chocolate in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the HOT FUDGE Only longline formal coat in chocolate.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14411.97 ms\n",
      "llama_perf_context_print: prompt eval time =   12932.90 ms /  3167 tokens (    4.08 ms per token,   244.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7424.55 ms /   170 runs   (   43.67 ms per token,    22.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   21891.94 ms /  3337 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Only longline formal coat in chocolate: The HOT FUDGE Only longline formal coat in chocolate, as seen in the image, exudes a sophisticated and classic style. It is made of a soft and luxurious fabric that gives it a plush and cozy texture, making it very comfortable to wear. The coat's longline design and the way it drapes suggest a relaxed fit, allowing for ample mobility. The rich, chocolate brown color of the coat makes it suitable for various weather conditions, from cooler days to light rain. It is ideal for formal occasions, business attire, and even casual settings when paired with the right accessories. The coat pairs well with a variety of looks, such as jeans and sneakers for a casual feel, high heels for a touch of elegance, or a trench coat for a more sophisticated ensemble.\n",
      "Image already exists: ./images_data/Bershka_longline_shearling_coat_in_brown.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_longline_shearling_coat_in_brown.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN Bershka longline shearling coat in brown in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN Bershka longline shearling coat in brown.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14211.01 ms\n",
      "llama_perf_context_print: prompt eval time =   12743.68 ms /  3173 tokens (    4.02 ms per token,   248.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6876.71 ms /   161 runs   (   42.71 ms per token,    23.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   21141.56 ms /  3334 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka longline shearling coat in brown: The brown Bershka longline shearling coat in the image is a stylish and comfortable piece of outerwear. Made of soft, plush material, it exudes a warm and inviting texture that would be perfect for cooler weather. Its loose fit allows for easy movement and a cozy, snug feel. The coat's brown color offers a versatile and timeless aesthetic that would pair well with a variety of outfits, from casual jeans and sneakers to more formal attire. Its length and shearling fabric make it an ideal choice for both comfort and style during the colder months. The coat is versatile enough to be worn for various occasions, from casual outings to business meetings, making it a practical addition to any wardrobe.\n",
      "Image already exists: ./images_data/Forever_New_formal_cocoon_coat_in_emerald_green.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Forever_New_formal_cocoon_coat_in_emerald_green.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Emerald Green Forever New formal cocoon coat in emerald green in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Emerald Green Forever New formal cocoon coat in emerald green.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14586.70 ms\n",
      "llama_perf_context_print: prompt eval time =   13150.20 ms /  3171 tokens (    4.15 ms per token,   241.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8115.21 ms /   185 runs   (   43.87 ms per token,    22.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   22760.06 ms /  3356 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Forever New formal cocoon coat in emerald green: The Emerald Green Forever New formal cocoon coat in the image exudes a sense of style and comfort. Its fabric appears to be of high quality, with a soft texture that looks both comfortable and warm. The coat's loose fit allows for a relaxed mobility, making it ideal for various weather conditions. Its emerald green color is striking and stands out, making it suitable for both formal and casual occasions. Paired with denim jeans and sneakers, it creates a laid-back yet stylish look, while with high heels and a trench coat, it becomes perfect for a chic, business-like appearance. The coat's overall aesthetic suggests it could be worn for a variety of events, from casual outings to more formal gatherings. Its design and color make it a versatile piece that can easily fit into one's wardrobe, offering a range of styling options.\n",
      "Image already exists: ./images_data/Noisy_May_Petite_longline_padded_coat_with_hood_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Noisy_May_Petite_longline_padded_coat_with_hood_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Noisy May Petite longline padded coat with hood in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Noisy May Petite longline padded coat with hood in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15360.42 ms\n",
      "llama_perf_context_print: prompt eval time =   13870.04 ms /  3175 tokens (    4.37 ms per token,   228.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6658.10 ms /   149 runs   (   44.69 ms per token,    22.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   22073.86 ms /  3324 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Noisy May Petite longline padded coat with hood in black: The black Noisy May Petite longline padded coat with hood in the image is a stylish and chic addition to any wardrobe. The coat is made of a soft fabric, providing a comfortable fit and ample warmth. Its padded design ensures mobility, while the hood offers protection from the elements. This coat is suitable for various weather conditions, including cold, rainy days. For casual occasions, pairing it with jeans and sneakers would complement the overall aesthetic. For more formal events, it could be worn with high heels and paired with a trench coat for a sophisticated look. Its versatility makes it suitable for various occasions, from everyday use to outdoor adventures and even business attire.\n",
      "Image already exists: ./images_data/Only_trench_coat_in_brown_check.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Only_trench_coat_in_brown_check.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN CHECK Only trench coat in brown check in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN CHECK Only trench coat in brown check.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15271.14 ms\n",
      "llama_perf_context_print: prompt eval time =   13807.27 ms /  3165 tokens (    4.36 ms per token,   229.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10076.46 ms /   212 runs   (   47.53 ms per token,    21.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   25417.67 ms /  3377 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Only trench coat in brown check: The trench coat in the image is a beautiful piece of clothing, showcasing a brown check pattern. The fabric of the coat appears to be soft and lightweight, suggesting a comfortable and breathable fit. The style of the coat is classic and versatile, with long sleeves and a belted waist, making it suitable for a variety of weather conditions. The overall aesthetic of the trench coat is elegant and timeless, with its tailored design and detailed pattern. The fit of the coat is relaxed yet stylish, allowing for ease of movement while still maintaining an aesthetically pleasing silhouette. This trench coat is ideal for various occasions such as casual, formal, date nights, and outdoor wear. Paired with jeans and sneakers, it exudes a casual vibe, while with high heels and a skirt, it becomes an elegant choice for a formal event or business attire. The trench coat's versatility and timeless appeal make it a must-have addition to any wardrobe.\n",
      "Image already exists: ./images_data/Topshop_Dad_denim_jacket_in_bleach.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Dad_denim_jacket_in_bleach.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Bleach Topshop Dad denim jacket in bleach in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Bleach Topshop Dad denim jacket in bleach.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15203.13 ms\n",
      "llama_perf_context_print: prompt eval time =   13680.37 ms /  3167 tokens (    4.32 ms per token,   231.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7169.90 ms /   160 runs   (   44.81 ms per token,    22.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   22446.64 ms /  3327 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Dad denim jacket in bleach: In the image, a person is seen wearing a distinctive Bleach Topshop Dad denim jacket in bleach. The jacket stands out with its unique fabric and style. The denim material is soft and comfortable, providing both style and warmth. The bleached color gives it a casual and trendy look, suitable for various occasions from casual outings to relaxed date nights. The jacket's loose fit and relaxed style make it ideal for everyday wear. Pairing it with jeans or sneakers would complete a stylish and comfortable outfit, while a trench coat can add a touch of formality to the ensemble. The overall aesthetic of the Bleach Topshop Dad denim jacket in bleach is versatile and perfect for a wide range of weather conditions and fashion occasions.\n",
      "Image already exists: ./images_data/Topshop_Tailored_double_layered_funnel_neck_trench_in_sage.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Tailored_double_layered_funnel_neck_trench_in_sage.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Khaki Topshop Tailored double layered funnel neck trench in sage in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Khaki Topshop Tailored double layered funnel neck trench in sage.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15226.80 ms\n",
      "llama_perf_context_print: prompt eval time =   13707.92 ms /  3177 tokens (    4.31 ms per token,   231.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5492.96 ms /   128 runs   (   42.91 ms per token,    23.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   20772.56 ms /  3305 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Tailored double layered funnel neck trench in sage: This image showcases a Khaki Topshop Tailored double layered funnel neck trench coat in sage. The coat is characterized by its soft fabric, providing comfort and a relaxed fit that allows for a high level of mobility. The double layered design and funnel neck contribute to the coat's overall aesthetic, making it a versatile piece for various weather conditions. It is suitable for casual, formal, and outdoor wear, as well as business attire. Pairing it with jeans, sneakers, or high heels can enhance its versatility, making it an ideal choice for a variety of occasions.\n",
      "Image already exists: ./images_data/Vero_Moda_Tall_coated_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Vero_Moda_Tall_coated_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Vero Moda Tall coated jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Vero Moda Tall coated jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14411.76 ms\n",
      "llama_perf_context_print: prompt eval time =   12891.51 ms /  3167 tokens (    4.07 ms per token,   245.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10211.60 ms /   235 runs   (   43.45 ms per token,    23.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   24683.20 ms /  3402 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Vero Moda Tall coated jacket in black: The BLACK Vero Moda Tall coated jacket is a striking piece of outerwear that exudes a sense of style and sophistication. The fabric appears to be a high-quality, possibly waterproof material that suggests both durability and comfort. The style of the jacket is reminiscent of classic trench coats, with its wide lapels and double-breasted closure, providing a flattering fit that is both relaxed and elegant. The jacket's overall aesthetic is completed by its belted waist, which accentuates the silhouette and gives a more tailored appearance. The material seems soft and flexible, allowing for a good range of mobility. This jacket would be suitable for a variety of weather conditions, from crisp autumn days to cooler evenings. It would be best suited for occasions such as casual outings, date nights, or even business attire, with its versatile and polished design. Pairing this jacket with dark jeans, sneakers, or high heels would create a chic and stylish look, while matching it with a trench coat could elevate the outfit for more formal occasions.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_washed_premium_real_leather_biker_jacket_in_grey.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_washed_premium_real_leather_biker_jacket_in_grey.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Grey ASOS DESIGN washed premium real leather biker jacket in grey in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Grey ASOS DESIGN washed premium real leather biker jacket in grey.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14556.89 ms\n",
      "llama_perf_context_print: prompt eval time =   13063.30 ms /  3171 tokens (    4.12 ms per token,   242.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7033.70 ms /   164 runs   (   42.89 ms per token,    23.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   21645.43 ms /  3335 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN washed premium real leather biker jacket in grey: This image showcases a Grey ASOS DESIGN washed premium real leather biker jacket, exuding a rugged and stylish vibe. The fabric appears to be soft and flexible, suggesting a high level of comfort for the wearer. The overall aesthetic of the jacket is casual and edgy, making it ideal for a variety of occasions such as date nights, outdoor wear, and casual events.\n",
      " \n",
      " The jacket's loose fit and relaxed sleeves ensure ample mobility, while its washed texture adds a touch of vintage charm. Pairing this jacket with black pants, sneakers, or even high heels would create a balanced look, complementing the jacket's unique design. Overall, this jacket is not only fashionable but also versatile, suitable for different weather conditions and styles.\n",
      "Image already exists: ./images_data/River_Island_oversized_coat_in_cream.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/River_Island_oversized_coat_in_cream.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the CREAM River Island oversized coat in cream in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the CREAM River Island oversized coat in cream.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14385.18 ms\n",
      "llama_perf_context_print: prompt eval time =   12870.50 ms /  3161 tokens (    4.07 ms per token,   245.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4253.61 ms /   100 runs   (   42.54 ms per token,    23.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   18688.19 ms /  3261 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for River Island oversized coat in cream: The cream River Island oversized coat in the image is characterized by its loose and comfortable fit, offering a high level of mobility. Its fabric has a soft and light texture, making it suitable for a wide range of weather conditions. This coat is best suited for casual occasions and pairing well with jeans and sneakers for a laid-back look. For a more formal or sophisticated style, it can be paired with high heels or trench coat for a stylish and chic appearance.\n",
      "Image already exists: ./images_data/Topshop_Tilda_western_denim_jacket_in_mid_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Tilda_western_denim_jacket_in_mid_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Mid Blue Topshop Tilda western denim jacket in mid blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Mid Blue Topshop Tilda western denim jacket in mid blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14820.82 ms\n",
      "llama_perf_context_print: prompt eval time =   13316.71 ms /  3169 tokens (    4.20 ms per token,   237.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7433.14 ms /   172 runs   (   43.22 ms per token,    23.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   22310.33 ms /  3341 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Tilda western denim jacket in mid blue: The Mid Blue Topshop Tilda western denim jacket in the image presents a striking mid blue hue that instantly catches the eye. Its style is reminiscent of the classic western denim, characterized by its distinctive button-up front, two chest pockets, and a collar. The jacket is tailored to fit loose, offering a comfortable and relaxed fit that allows for freedom of movement. The fabric appears to be a soft cotton, giving it a comfortable texture that would be ideal for a range of weather conditions from casual summer days to milder weather. It pairs well with jeans, sneakers, and high heels, making it suitable for both casual and smart-casual occasions. The jacket's versatility and timeless style make it a versatile piece that can easily transition from a casual day out to a more formal event.\n",
      "Image already exists: ./images_data/Topshop_faux_leather_trench_coat_in_dark_chocolate.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_faux_leather_trench_coat_in_dark_chocolate.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN Topshop faux leather trench coat in dark chocolate in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN Topshop faux leather trench coat in dark chocolate.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14362.06 ms\n",
      "llama_perf_context_print: prompt eval time =   12843.85 ms /  3171 tokens (    4.05 ms per token,   246.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7367.58 ms /   172 runs   (   42.83 ms per token,    23.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   21784.05 ms /  3343 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop faux leather trench coat in dark chocolate: The image showcases a brown Topshop faux leather trench coat in a dark chocolate color. It's made of a smooth, faux leather material, which gives it a luxurious and stylish look. The coat is loose-fitting, suggesting a comfortable and roomy wear that allows for ease of movement. The dark chocolate color is both elegant and versatile, making it suitable for a variety of weather conditions, from casual to formal occasions. For casual outings, it pairs well with jeans and sneakers, while for more formal events, it complements high heels and formal dresses. It's an ideal choice for date night or business attire, adding a touch of sophistication to any look. The coat's texture and style make it a perfect accessory for any season, providing both style and functionality.\n",
      "Image already exists: ./images_data/NA-KD_x_Angelica_Blick_faux_leather_trench_coat_in_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/NA-KD_x_Angelica_Blick_faux_leather_trench_coat_in_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLUE NA-KD x Angelica Blick faux leather trench coat in blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLUE NA-KD x Angelica Blick faux leather trench coat in blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15162.05 ms\n",
      "llama_perf_context_print: prompt eval time =   13671.65 ms /  3181 tokens (    4.30 ms per token,   232.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5944.12 ms /   138 runs   (   43.07 ms per token,    23.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   21159.00 ms /  3319 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for NA-KD x Angelica Blick faux leather trench coat in blue: The trench coat in the image is a vibrant blue NA-KD x Angelica Blick faux leather number. It's a loose-fitting coat, providing ample comfort and freedom of movement. The faux leather texture gives it a luxurious and stylish feel, while its light blue color stands out beautifully against the white background. The coat's design and material make it suitable for various weather conditions, from cool autumn days to mild spring evenings. It pairs well with a variety of outfits, from casual jeans and sneakers to more formal looks, making it versatile for different occasions, from casual outings to date nights.\n",
      "Image already exists: ./images_data/Crooked_Tongues_x_Felix_the_Cat_Unisex_varsity_jacket_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Crooked_Tongues_x_Felix_the_Cat_Unisex_varsity_jacket_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLACK Crooked Tongues x Felix the Cat Unisex varsity jacket in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLACK Crooked Tongues x Felix the Cat Unisex varsity jacket in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15130.49 ms\n",
      "llama_perf_context_print: prompt eval time =   13592.43 ms /  3181 tokens (    4.27 ms per token,   234.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10584.03 ms /   241 runs   (   43.92 ms per token,    22.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   25776.12 ms /  3422 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Crooked Tongues x Felix the Cat Unisex varsity jacket in black: The black Crooked Tongues x Felix the Cat Unisex varsity jacket in the image is a standout piece of clothing. Its fabric is a mix of cotton and wool, giving it a soft texture while maintaining some level of durability. The style is reminiscent of a classic varsity jacket, complete with a black ribbed collar and black ribbed cuffs. A unique feature of the jacket is the Felix the Cat emblem on the front, adding a touch of whimsy to the otherwise edgy design. The overall aesthetic of the jacket is a blend of streetwear and vintage sports-inspired fashion.\n",
      " \n",
      " The fit of the jacket is relaxed and comfortable, allowing for ample mobility. This makes it suitable for a variety of weather conditions, from cooler days to slightly chilly nights. Pairing the jacket with jeans, sneakers, or high heels would create a stylish casual look, while wearing it over a trench coat or with dress pants could elevate it for more formal occasions. The versatility of this jacket makes it an ideal choice for various settings, from casual outings to more formal gatherings.\n",
      "Image already exists: ./images_data/Topshop_faux_leather_cropped_car_coat_with_faux_fur_trims_in_chocolate.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_faux_leather_cropped_car_coat_with_faux_fur_trims_in_chocolate.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Chocolate Topshop faux leather cropped car coat with faux fur trims in chocolate in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Chocolate Topshop faux leather cropped car coat with faux fur trims in chocolate.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14663.68 ms\n",
      "llama_perf_context_print: prompt eval time =   13165.47 ms /  3181 tokens (    4.14 ms per token,   241.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8683.50 ms /   200 runs   (   43.42 ms per token,    23.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   23404.17 ms /  3381 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop faux leather cropped car coat with faux fur trims in chocolate: This image features a woman wearing a Chocolate Topshop faux leather cropped car coat with faux fur trims in chocolate. The coat has a stylish and contemporary design, with its cropped length and car-style hood adding a unique touch to the overall aesthetic. The faux leather fabric gives the coat a luxurious feel, while the faux fur trims add a touch of warmth and softness. The fit of the coat appears to be relaxed, providing a comfortable and roomy fit. The material and texture of the coat suggest it is suitable for various weather conditions, from crisp autumn days to milder winter nights. This versatile piece is best suited for a range of occasions, from casual outings to formal events, making it an excellent choice for anyone looking to upgrade their wardrobe. It pairs well with jeans and sneakers for a casual look or can be paired with high heels or a trench coat for a more refined and elegant look.\n",
      "Image already exists: ./images_data/Bershka_tailored_coat_in_blue.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Bershka_tailored_coat_in_blue.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLUE Bershka tailored coat in blue in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLUE Bershka tailored coat in blue.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14857.25 ms\n",
      "llama_perf_context_print: prompt eval time =   13357.22 ms /  3165 tokens (    4.22 ms per token,   236.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5769.74 ms /   133 runs   (   43.38 ms per token,    23.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   20680.53 ms /  3298 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka tailored coat in blue: The BLUE Bershka tailored coat in the image exudes a modern and stylish look. Its fabric appears to be a lightweight and breathable material, suitable for a variety of weather conditions. The style of the coat is quite unique with its tailored fit and flared sleeves, giving it a chic and fashionable appearance. The material of the coat seems to be soft and comfortable, suggesting that it's ideal for both casual and formal occasions. It pairs well with jeans for a relaxed look or high heels for a more formal attire. Overall, the coat is versatile and suitable for various weather conditions and occasions.\n",
      "Image already exists: ./images_data/Bershka_faux_leather_racer_jacket_in_blue_&_white.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image './images_data/Bershka_faux_leather_racer_jacket_in_blue_&_white.jpg' -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BLUE Bershka faux leather racer jacket in blue & white in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BLUE Bershka faux leather racer jacket in blue & white.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14543.32 ms\n",
      "llama_perf_context_print: prompt eval time =   13040.29 ms /  3175 tokens (    4.11 ms per token,   243.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7816.75 ms /   181 runs   (   43.19 ms per token,    23.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   22415.54 ms /  3356 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Bershka faux leather racer jacket in blue & white: The image presents a woman showcasing a blue Bershka faux leather racer jacket in a light blue and white color. The jacket appears to be made from a soft, pliable fabric, suggesting a comfortable fit. Its style is reminiscent of a classic leather jacket, albeit with a faux leather finish. The jacket's racer design is accentuated by its white stripe running down the back, adding a touch of contrast to the overall aesthetic.\n",
      " \n",
      " The fit of the jacket seems to be relaxed, allowing for a comfortable wear. Its material and texture seem suitable for various weather conditions, making it a versatile addition to a wardrobe. The jacket's design and color scheme make it ideal for casual wear and outdoor activities. Its style also suggests that it might pair well with jeans, sneakers, or high heels, depending on the occasion.\n",
      "Image already exists: ./images_data/Selected_Femme_long_line_trench_coat_in_beige.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Selected_Femme_long_line_trench_coat_in_beige.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Beige Selected Femme long line trench coat in beige in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Beige Selected Femme long line trench coat in beige.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15126.95 ms\n",
      "llama_perf_context_print: prompt eval time =   13600.68 ms /  3171 tokens (    4.29 ms per token,   233.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7879.95 ms /   184 runs   (   42.83 ms per token,    23.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   23062.93 ms /  3355 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Selected Femme long line trench coat in beige: The Beige Selected Femme long line trench coat in beige in this image is made of a smooth, lightweight fabric that offers a comfortable fit. Its loose, relaxed style is complemented by the long sleeves and its long line design. The trench coat is made of a material that appears soft and comfortable, suggesting it's suitable for various weather conditions. Its design makes it versatile, suitable for both casual and formal occasions, as well as outdoor wear and business attire. The trench coat pairs well with jeans, sneakers, or high heels, and it could be a stylish choice for a date night. The woman in the image has chosen to wear the trench coat with gray pants and white shoes, creating a balanced and fashionable look. The trench coat's neutral color and versatile style make it a great addition to any wardrobe.\n",
      "Image already exists: ./images_data/Only_Tall_longline_tailored_coat_in_stone.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Only_Tall_longline_tailored_coat_in_stone.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the STONE Only Tall longline tailored coat in stone in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the STONE Only Tall longline tailored coat in stone.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14238.73 ms\n",
      "llama_perf_context_print: prompt eval time =   12743.67 ms /  3165 tokens (    4.03 ms per token,   248.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7045.99 ms /   163 runs   (   43.23 ms per token,    23.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   21340.03 ms /  3328 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Only Tall longline tailored coat in stone: The STONE Only Tall longline tailored coat in stone featured in the image is a versatile and elegant piece of outerwear. The fabric is a blend of cotton and wool, providing a soft and comfortable texture that is ideal for colder weather conditions. Its style is sophisticated and modern, featuring a relaxed fit that is both stylish and functional. The coat is designed with a high neck and long sleeves, offering both warmth and protection from the elements. Its overall aesthetic is perfect for a range of occasions, from casual to formal, and it pairs well with both jeans and leather pants, as well as high heels or sneakers. The woman in the image is seen sitting on a white surface, demonstrating the coat's mobility and its potential as a versatile wardrobe staple.\n",
      "Image already exists: ./images_data/Pull&Bear_oversized_motorcycle_jacket_in_navy_with_contrast_navy_and_yellow_pannels.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image './images_data/Pull&Bear_oversized_motorcycle_jacket_in_navy_with_contrast_navy_and_yellow_pannels.jpg' -c 4096 -p \"\n",
      "USER:\n",
      "Describe the NAVY Pull&Bear oversized motorcycle jacket in navy with contrast navy and yellow pannels in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the NAVY Pull&Bear oversized motorcycle jacket in navy with contrast navy and yellow pannels.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14394.92 ms\n",
      "llama_perf_context_print: prompt eval time =   12883.98 ms /  3185 tokens (    4.05 ms per token,   247.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5467.85 ms /   127 runs   (   43.05 ms per token,    23.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   19914.57 ms /  3312 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Pull&Bear oversized motorcycle jacket in navy with contrast navy and yellow pannels: The NAVY Pull&Bear oversized motorcycle jacket in navy with contrast navy and yellow panels stands out as a fashionable piece of clothing. Its fabric appears to be a mix of cotton and leather, giving it a soft yet durable feel. The style is reminiscent of biker attire, characterized by its oversized fit that is both loose and comfortable. The jacket is suitable for cool weather conditions, providing warmth while still allowing for good mobility. It is best suited for casual occasions or outdoor wear, and would pair well with jeans and sneakers, adding a touch of style and personality to any outfit.\n",
      "Image already exists: ./images_data/Missguided_faux_leather_puffer_jacket_in_white.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Missguided_faux_leather_puffer_jacket_in_white.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the White Missguided faux leather puffer jacket in white in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the White Missguided faux leather puffer jacket in white.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14859.69 ms\n",
      "llama_perf_context_print: prompt eval time =   13300.55 ms /  3167 tokens (    4.20 ms per token,   238.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6811.31 ms /   158 runs   (   43.11 ms per token,    23.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   21724.59 ms /  3325 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Missguided faux leather puffer jacket in white: The White Missguided faux leather puffer jacket in white is a versatile and stylish addition to any wardrobe. Its faux leather texture adds a touch of sophistication, while the puffer style provides a cozy and warm feel. The jacket appears to be a medium fit, offering a comfortable balance between style and functionality. Its material and texture seem to be soft yet durable, suggesting it could be suitable for casual or relaxed occasions. The overall aesthetic of the jacket is elegant and fashionable, making it a great choice for date nights, outdoor wear, or even casual outings. It pairs well with various clothing items, from jeans and sneakers to high heels or a trench coat, making it a versatile piece that can easily blend into different styles.\n",
      "Image already exists: ./images_data/Reclaimed_Vintage_longline_puffer_coat_with_faux_fur_detail_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Reclaimed_Vintage_longline_puffer_coat_with_faux_fur_detail_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black Reclaimed Vintage longline puffer coat with faux fur detail in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black Reclaimed Vintage longline puffer coat with faux fur detail in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14452.83 ms\n",
      "llama_perf_context_print: prompt eval time =   12930.53 ms /  3177 tokens (    4.07 ms per token,   245.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5894.83 ms /   137 runs   (   43.03 ms per token,    23.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   20399.88 ms /  3314 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Reclaimed Vintage longline puffer coat with faux fur detail in black: The Black Reclaimed Vintage longline puffer coat with faux fur detail in black is a striking piece of fashion. Its fabric appears to be a soft, lightweight material, offering a level of comfort and mobility that is ideal for daily activities. The style is reminiscent of vintage fashion, with a puffed silhouette that is both trendy and timeless. The coat's pattern is solid black, providing a bold statement, and the faux fur detail adds a touch of luxury and warmth. This coat is best suited for colder weather conditions, making it perfect for casual or formal occasions, and it pairs well with both jeans and high heels.\n",
      "Image already exists: ./images_data/New_Look_Tall_mid_length_hooded_puffer_coat_in_black.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/New_Look_Tall_mid_length_hooded_puffer_coat_in_black.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Black New Look Tall mid length hooded puffer coat in black in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Black New Look Tall mid length hooded puffer coat in black.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14980.33 ms\n",
      "llama_perf_context_print: prompt eval time =   13483.86 ms /  3169 tokens (    4.25 ms per token,   235.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5781.80 ms /   134 runs   (   43.15 ms per token,    23.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   20814.20 ms /  3303 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for New Look Tall mid length hooded puffer coat in black: The Black New Look Tall mid length hooded puffer coat in this image exudes a stylish, urban vibe. The puffy design and drawstring hood give it a trendy, yet comfortable feel. Made of a soft and warm fabric, it provides both insulation and flexibility, making it suitable for a range of weather conditions from light snow to chilly winds. Its versatile nature allows it to pair well with various styles, from casual to formal, making it a versatile piece for different occasions. Whether you're headed out for a casual day or dressing up for a night out, this coat will keep you looking stylish and comfortable.\n",
      "Image already exists: ./images_data/In_The_Style_belted_trench_coat_in_beige.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/In_The_Style_belted_trench_coat_in_beige.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BEIGE In The Style belted trench coat in beige in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BEIGE In The Style belted trench coat in beige.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14796.41 ms\n",
      "llama_perf_context_print: prompt eval time =   13268.43 ms /  3171 tokens (    4.18 ms per token,   238.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6864.08 ms /   159 runs   (   43.17 ms per token,    23.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   21714.31 ms /  3330 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for In The Style belted trench coat in beige: The beige In The Style belted trench coat in the image has a classic and elegant style. Its fabric appears to be of a high quality, with a smooth and luxurious texture. The coat is designed in a loose and comfortable fit, which suggests a high level of mobility. The material of the coat is likely waterproof, making it suitable for various weather conditions. Its versatility makes it ideal for a range of occasions, from casual outings to formal events and business settings. Paired with jeans, the coat exudes a casual yet stylish vibe, while with high heels, it adds a touch of sophistication to a formal look. The beige color of the coat can also be easily dressed up or down, depending on the rest of the outfit.\n",
      "Image already exists: ./images_data/Topshop_Tall_chuck_on_coat_in_oat.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Topshop_Tall_chuck_on_coat_in_oat.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the OAT Topshop Tall chuck on coat in oat in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the OAT Topshop Tall chuck on coat in oat.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15194.42 ms\n",
      "llama_perf_context_print: prompt eval time =   13673.97 ms /  3165 tokens (    4.32 ms per token,   231.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6524.26 ms /   152 runs   (   42.92 ms per token,    23.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   21771.67 ms /  3317 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Topshop Tall chuck on coat in oat: The OAT Topshop Tall chuck on coat in oat, as seen in the image, is a versatile piece that exudes a casual yet stylish vibe. The coat is made of a soft fabric, providing both comfort and warmth. Its loose fit allows for easy mobility, while the belted waist adds a touch of sophistication. The coat's oat color is versatile, making it suitable for various weather conditions, ranging from chilly autumn days to mild winter evenings. Pairing this coat with blue jeans and black ankle boots would make for a chic, casual outfit, while layering it over a formal dress or pairing it with high heels would elevate it for a sophisticated night out.\n",
      "Image already exists: ./images_data/COLLUSION_shaggy_faux_mongolian_fur_jacket_in_cream.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/COLLUSION_shaggy_faux_mongolian_fur_jacket_in_cream.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the CREAM COLLUSION shaggy faux mongolian fur jacket in cream in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the CREAM COLLUSION shaggy faux mongolian fur jacket in cream.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14943.82 ms\n",
      "llama_perf_context_print: prompt eval time =   13399.76 ms /  3179 tokens (    4.22 ms per token,   237.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7374.93 ms /   171 runs   (   43.13 ms per token,    23.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   22372.92 ms /  3350 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for COLLUSION shaggy faux mongolian fur jacket in cream: The CREAM COLLUSION shaggy faux mongolian fur jacket in cream is a statement piece that exudes a luxurious and cozy vibe. The fabric is soft and plush, with the faux fur adding a touch of warmth and texture. Its loose fit allows for a comfortable and relaxed wear, while the lightweight material ensures good mobility. This jacket is suitable for a variety of weather conditions, from chilly autumn nights to milder spring days, and it can be dressed up or down depending on the occasion. Pair it with jeans and sneakers for a casual look or with high heels and a trench coat for a more formal and sophisticated style. The cream color of the jacket can easily complement a range of outfits, making it a versatile and timeless addition to any wardrobe.\n",
      "Image already exists: ./images_data/Stradivarius_tailored_belted_coat_in_camel.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_tailored_belted_coat_in_camel.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the CAMEL Stradivarius tailored belted coat in camel in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the CAMEL Stradivarius tailored belted coat in camel.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14464.38 ms\n",
      "llama_perf_context_print: prompt eval time =   12957.74 ms /  3173 tokens (    4.08 ms per token,   244.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6725.15 ms /   157 runs   (   42.84 ms per token,    23.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   21242.53 ms /  3330 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius tailored belted coat in camel: The CAMEL Stradivarius tailored belted coat in the image exudes a luxurious and stylish aesthetic. The coat is made of a high-quality, soft fabric that gives it a comfortable and cozy feel. Its belted design provides a snug and flattering fit, perfect for both casual and formal occasions. The coat is ideal for various weather conditions, including colder days and breezy evenings, providing both warmth and protection. It pairs well with jeans for a casual look, with high heels for a sophisticated style, or even with a trench coat for a layered, polished ensemble. Its versatile design and timeless color make it a versatile piece that can be dressed up or down for a wide range of occasions.\n",
      "Image already exists: ./images_data/Stradivarius_brushed_shacket_in_ecru.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Stradivarius_brushed_shacket_in_ecru.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the ECRU Stradivarius brushed shacket in ecru in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the ECRU Stradivarius brushed shacket in ecru.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15084.68 ms\n",
      "llama_perf_context_print: prompt eval time =   13526.41 ms /  3169 tokens (    4.27 ms per token,   234.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6190.69 ms /   143 runs   (   43.29 ms per token,    23.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   21327.91 ms /  3312 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Stradivarius brushed shacket in ecru: The image showcases a stylish individual in an ecru Stradivarius brushed shacket. The shacket is characterized by its loose fit and soft, brushed texture, which gives it a cozy and comfortable feel. Its light gray color and casual style make it ideal for various occasions, from relaxed days out to casual evenings. The shacket would pair well with jeans, sneakers, or even high heels, adding an elegant touch to any outfit. Its breathable fabric suggests that it is suitable for a range of weather conditions, from mild to slightly chilly days. Overall, this ecru Stradivarius brushed shacket offers a versatile and stylish addition to any wardrobe.\n",
      "Image already exists: ./images_data/Pimkie_double_breasted_longline_coat_in_brown.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/Pimkie_double_breasted_longline_coat_in_brown.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the BROWN Pimkie double breasted longline coat in brown in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the BROWN Pimkie double breasted longline coat in brown.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   15147.22 ms\n",
      "llama_perf_context_print: prompt eval time =   13617.21 ms /  3171 tokens (    4.29 ms per token,   232.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7176.79 ms /   166 runs   (   43.23 ms per token,    23.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   22379.44 ms /  3337 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for Pimkie double breasted longline coat in brown: This image features a woman standing in front of a white background. She is wearing a brown Pimkie double breasted longline coat that exudes a sophisticated and stylish vibe. The coat is made of a smooth, soft fabric, perfect for both comfort and style. Its double-breasted design and long length suggest a relaxed fit, while the brown hue gives it an elegant and timeless appeal. The coat would be suitable for a variety of weather conditions, from casual to formal occasions. Paired with jeans, sneakers, or high heels, it would be a versatile choice for date nights, outdoor activities, or even business attire. The overall aesthetic of the coat, combined with the woman's confident stance, makes it an eye-catching accessory in the image.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_longline_trench_coat_in_khaki.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_longline_trench_coat_in_khaki.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the Khaki ASOS DESIGN longline trench coat in khaki in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the Khaki ASOS DESIGN longline trench coat in khaki.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14808.56 ms\n",
      "llama_perf_context_print: prompt eval time =   13343.51 ms /  3173 tokens (    4.21 ms per token,   237.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8412.04 ms /   191 runs   (   44.04 ms per token,    22.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   23278.64 ms /  3364 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN longline trench coat in khaki: The Khaki ASOS DESIGN longline trench coat in khaki in the image is a versatile, stylish piece that exudes a relaxed yet elegant vibe. Its fabric, a mix of cotton and wool, gives it a soft, comfortable texture, perfect for cooler weather conditions. The trench coat's loose fit, with its oversized silhouette, allows for a wide range of mobility and comfort. The material appears to be waterproof, making it an ideal choice for rainy days or outdoor activities. Its timeless design and the neutral color make it suitable for various occasions such as casual, formal, date night, and business attire. Paired with jeans, sneakers, high heels, or even trench coats, the Khaki ASOS DESIGN longline trench coat in khaki stands out as a versatile, functional, and fashionable addition to any wardrobe.\n",
      "Image already exists: ./images_data/ASOS_DESIGN_Petite_rubberised_rain_parka_in_stone.jpg\n",
      "Running Command: /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli -m /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf --mmproj /Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf --image ./images_data/ASOS_DESIGN_Petite_rubberised_rain_parka_in_stone.jpg -c 4096 -p \"\n",
      "USER:\n",
      "Describe the stone ASOS DESIGN Petite rubberised rain parka in stone in this image in detail.\n",
      "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
      "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
      "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
      "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
      "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
      "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
      "- Ignore other clothing items in the image and focus only on the stone ASOS DESIGN Petite rubberised rain parka in stone.\n",
      "- Do NOT add any extra information outside the description.\n",
      "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
      "\n",
      "ASSISTANT:\n",
      "\" > llava_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: build: 3943 (cda0e4b6) with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0\n",
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral 7B Instruct v0.2\n",
      "llama_model_loader: - kv   2:                            general.version str              = v0.2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Mistralai\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv   9:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = Mistral 7B Instruct v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_perf_context_print:        load time =   14575.51 ms\n",
      "llama_perf_context_print: prompt eval time =   13126.16 ms /  3171 tokens (    4.14 ms per token,   241.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7808.74 ms /   177 runs   (   44.12 ms per token,    22.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   22451.49 ms /  3348 tokens\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "Generated Description for ASOS DESIGN Petite rubberised rain parka in stone: The stone ASOS DESIGN Petite rubberised rain parka is a beige-colored jacket that exudes a stylish, casual vibe. Its fabric is a blend of soft cotton and thick wool, providing both comfort and durability. The jacket's style is chic and practical, with a distinctive raincoat design that features a high collar for protection against the elements. The material is waterproof, ensuring the wearer stays dry in wet conditions.\n",
      " \n",
      " This jacket is best suited for casual outings or relaxed social gatherings. It pairs well with a variety of bottoms, including jeans, sneakers, or high heels, and can be dressed up for a more formal occasion with the addition of dress pants or a skirt. Its overall aesthetic is versatile and trendy, making it an ideal choice for a range of weather conditions and occasions.\n",
      "Descriptions generated and saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import shlex \n",
    "import re\n",
    "\n",
    "# Define LLaVA paths (modify these based on your system)\n",
    "LLAVA_CLI_PATH = \"/Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/llama-llava-cli\"\n",
    "LLAVA_MODEL_PATH = \"/Users/glennsuristio/Documents/Projects/dressAI/llava-v1.6-mistral-7b/Mistral-7B-Instruct-v0.2-F32-Q4_K_M.gguf\"\n",
    "MM_PROJ_PATH = \"/Users/glennsuristio/Documents/Projects/dressAI/llama.cpp/vit/mmproj-model-f16.gguf\"\n",
    "IMAGES_DATA = \"./images_data\"\n",
    "DESCRIPTIONS_DATA = \"./descriptions_data\"\n",
    "\n",
    "# Ensure temp directory exists\n",
    "os.makedirs(IMAGES_DATA, exist_ok=True)\n",
    "os.makedirs(DESCRIPTIONS_DATA, exist_ok=True)\n",
    "\n",
    "def download_image(image_url, filename):\n",
    "    \"\"\"Downloads an image from a URL if it doesn't already exist.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Image already exists: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    \"\"\"Downloads an image from a URL and saves it locally.\"\"\"\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "        return filename\n",
    "    else:\n",
    "        print(f\"Failed to download image: {image_url}\")\n",
    "        return None\n",
    "\n",
    "def extract_description(output_file):\n",
    "    \"\"\"Extracts only the relevant product description from LLaVA output.\"\"\"\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the starting point of the description\n",
    "    start_index = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"encode_image_with_clip: image encoded\" in line:\n",
    "            start_index = i + 1  # Description starts on the next line\n",
    "            break\n",
    "\n",
    "    # Extract everything after the start_index\n",
    "    if start_index is not None and start_index < len(lines):\n",
    "        return \" \".join(lines[start_index:]).strip()\n",
    "    else:\n",
    "        return \"Description not found\"\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Replaces special characters that are invalid in filenames.\"\"\"\n",
    "    name = name.replace(' ', '_')  # Replace spaces with underscores\n",
    "    return re.sub(r'[\\\\/:\"*?<>|]', '_', name)  # Replace `/ \\ : \" * ? < > |` with `_`\n",
    "\n",
    "def generate_description(image_url, name, color):\n",
    "    \"\"\"Generates a textual description using LLaVA for a given fashion product image.\"\"\"\n",
    "    safe_name = sanitize_filename(name)\n",
    "    image_path = os.path.join(IMAGES_DATA, f\"{safe_name}.jpg\")\n",
    "    description_path = os.path.join(DESCRIPTIONS_DATA, f\"{safe_name}.txt\")\n",
    "\n",
    "    # If description file already exists, read from it\n",
    "    if os.path.exists(description_path):\n",
    "        with open(description_path, \"r\") as file:\n",
    "            existing_description = file.read().strip()\n",
    "\n",
    "        if existing_description and existing_description != \"Description not found\":\n",
    "            print(f\"✅ Using existing description for {name}\")\n",
    "            return existing_description\n",
    "        else:\n",
    "            print(f\"🔄 Regenerating description for {name} (previously invalid)\")\n",
    "\n",
    "        # print(f\"Reading existing description for {name}\")\n",
    "        # return open(description_path, \"r\").read()\n",
    "\n",
    "    downloaded_image = download_image(image_url, image_path)\n",
    "    if not downloaded_image:\n",
    "        return \"Image not available\"\n",
    "    \n",
    "    output_file = \"llava_output.txt\"\n",
    "    \n",
    "#     prompt = f\"\"\"{image_path}\n",
    "# USER:\n",
    "# Describe the {color} {name} in this image in detail.\n",
    "# - Focus on its fabric, style, and patterns.\n",
    "# - Ignore other clothing items other than {color} {name} in the image.\n",
    "# - Do NOT add any extra information other than the description.\n",
    "# - Write the response as a single detailed paragraph. Do not use bullet points.\n",
    "# - Avoid listing features separately; instead, describe the product naturally in a flowing sentence.\n",
    "\n",
    "# ASSISTANT:\n",
    "# \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "USER:\n",
    "Describe the {color} {name} in this image in detail.\n",
    "- Focus on its **fabric, style, patterns, and overall aesthetic**.\n",
    "- Mention the **fit** (e.g., loose, tight, relaxed), **comfort level**, and **mobility**.\n",
    "- Describe the **material and texture** (e.g., soft cotton, thick wool, waterproof fabric).\n",
    "- Indicate whether it is **suitable for certain weather conditions** (e.g., breathable for summer, ideal for rainy days).\n",
    "- Suggest occasions it is best suited for (e.g., casual, formal, date night, outdoor wear, business attire).\n",
    "- Optionally mention what it might **pair well with** (e.g., jeans, sneakers, high heels, trench coat).\n",
    "- Ignore other clothing items in the image and focus only on the {color} {name}.\n",
    "- Do NOT add any extra information outside the description.\n",
    "- Write the response as a **single paragraph** with **natural, flowing sentences**.\n",
    "\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    command = f'{LLAVA_CLI_PATH} -m {LLAVA_MODEL_PATH} --mmproj {MM_PROJ_PATH} --image {shlex.quote(image_path)} -c 4096 -p \"{prompt}\" > {output_file}'\n",
    "    print(f\"Running Command: {command}\")  # Debugging\n",
    "    \n",
    "    process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    print(\"STDOUT:\", process.stdout)  # Debugging\n",
    "    print(\"STDERR:\", process.stderr)  # Debugging\n",
    "    \n",
    "    # Extract clean description\n",
    "    description = extract_description(output_file)\n",
    "\n",
    "    # Save the description to a file\n",
    "    with open(description_path, \"w\") as desc_file:\n",
    "        desc_file.write(description)\n",
    "        \n",
    "    print(f\"Generated Description for {name}: {description}\")\n",
    "    return description\n",
    "\n",
    "# Apply LLaVA on limited products\n",
    "df['description'] = df.apply(lambda row: generate_description(row['image link'], row['name'], row['color']), axis=1)\n",
    "df['image_path'] = df.apply(lambda row: os.path.join(IMAGES_DATA, f\"{row['name'].replace(' ', '_')}.jpg\"), axis=1)\n",
    "\n",
    "print(\"Descriptions generated and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: (takes time)\n",
    "- Generate embedding with MiniLM-L6-v2\n",
    "- Convert embeddings into FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 Creating new FAISS index...\n",
      "🆕 Found 100 new products. Updating FAISS index...\n",
      "✅ FAISS index updated with new products!\n",
      "🚀 FAISS index ready & embeddings stored in df!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Directory for FAISS index\n",
    "FAISS_DIR = \"faiss_data\"\n",
    "os.makedirs(FAISS_DIR, exist_ok=True)\n",
    "\n",
    "# File path for FAISS index\n",
    "FAISS_INDEX_FILE = os.path.join(FAISS_DIR, \"faiss_index.bin\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 1: Load FAISS index if it exists\n",
    "if os.path.exists(FAISS_INDEX_FILE):\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "    num_existing = index.ntotal  # Number of stored products in FAISS\n",
    "\n",
    "    # Retrieve stored embeddings only if FAISS has indexed products\n",
    "    if num_existing > 0:\n",
    "        existing_embeddings = np.zeros((num_existing, index.d), dtype=np.float32)\n",
    "        index.reconstruct_batch(np.arange(num_existing), existing_embeddings)  # Batch retrieval\n",
    "\n",
    "        # Assign stored embeddings back to df\n",
    "        df.loc[df.index[:num_existing], \"embedding\"] = pd.Series(list(existing_embeddings))\n",
    "\n",
    "    # Identify new products that don't have an index yet\n",
    "    new_products = df[df[\"embedding\"].isna()]\n",
    "\n",
    "else:\n",
    "    print(\"Creating new FAISS index...\")\n",
    "    index = None  # Placeholder for FAISS index\n",
    "    new_products = df  # All products are new\n",
    "\n",
    "# Step 2: Generate embeddings ONLY for new products\n",
    "if not new_products.empty:\n",
    "    print(f\"Found {len(new_products)} new products. Updating FAISS index...\")\n",
    "\n",
    "    # 🚀 **Batch Encode New Descriptions**\n",
    "    new_embeddings = embedding_model.encode(\n",
    "        new_products[\"description\"].tolist(),\n",
    "        batch_size=32,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(np.float32)  # Ensure correct dtype\n",
    "\n",
    "    new_embeddings = np.array(new_embeddings, dtype=np.float32)\n",
    "\n",
    "    # Store embeddings in df using `.loc`\n",
    "    df.loc[new_products.index, \"embedding\"] = pd.Series(list(new_embeddings), index=new_products.index)\n",
    "\n",
    "\n",
    "    # Convert to FAISS format\n",
    "    new_embeddings_array = np.vstack(df.loc[new_products.index, \"embedding\"].to_numpy())\n",
    "\n",
    "    new_embeddings_array = np.array(new_embeddings_array, dtype=np.float32)\n",
    "\n",
    "    # Add new embeddings to FAISS\n",
    "    if index is None:\n",
    "        index = faiss.IndexFlatL2(new_embeddings_array.shape[1])  # Create FAISS index\n",
    "    index.add(new_embeddings_array)\n",
    "\n",
    "    # Save updated FAISS index\n",
    "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
    "    print(\"FAISS index updated with new products!\")\n",
    "\n",
    "else:\n",
    "    print(\"No new products found. Using existing FAISS index.\")\n",
    "\n",
    "print(\"FAISS index ready & embeddings stored in df!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "- Retrieve top 3 products with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def retrieve_relevant_products(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval: Combines FAISS (dense) and TF-IDF (sparse) for better search.\n",
    "    \"\"\"\n",
    "    #Convert query to FAISS embedding\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).reshape(1, -1)\n",
    "\n",
    "    #Retrieve from FAISS\n",
    "    top_k_faiss = top_k * 3  # Retrieve more to rerank better\n",
    "    distances, indices = index.search(query_embedding.astype(np.float32), top_k_faiss)\n",
    "    retrieved_products = df.iloc[indices[0]].copy()  # Use .copy() to avoid modifying original df\n",
    "\n",
    "    #Compute cosine similarity for reranking\n",
    "    product_embeddings = np.vstack(retrieved_products[\"embedding\"].to_numpy())\n",
    "    similarity_scores = cosine_similarity(query_embedding, product_embeddings)[0]\n",
    "    retrieved_products[\"similarity\"] = similarity_scores\n",
    "\n",
    "    #Sparse Retrieval Using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"name\"] + \" \" + df[\"description\"])  # Use both name & description\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    #Compute TF-IDF similarity scores for retrieved products only\n",
    "    sparse_scores = np.array((tfidf_matrix[retrieved_products.index] @ query_vector.T).todense()).flatten()\n",
    "\n",
    "    #Assign sparse scores only to retrieved products\n",
    "    retrieved_products[\"sparse_score\"] = sparse_scores\n",
    "\n",
    "    # Normalize scores and rerank\n",
    "    retrieved_products[\"final_score\"] = (\n",
    "        0.7 * retrieved_products[\"similarity\"] +  # Dense search weight\n",
    "        0.3 * retrieved_products[\"sparse_score\"]  # Sparse search weight\n",
    "    )\n",
    "    retrieved_products = retrieved_products.sort_values(\"final_score\", ascending=False)\n",
    "\n",
    "    return retrieved_products.head(top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "- Generate rag text response\n",
    "- Pass in result details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama \n",
    "\n",
    "def generate_rag_response(user_query):\n",
    "    \"\"\"\n",
    "    Uses Ollama to generate structured product recommendations with explanations, styling tips, and alternative suggestions.\n",
    "    \"\"\"\n",
    "    retrieved_products = retrieve_relevant_products(user_query, top_k=3)\n",
    "\n",
    "    if retrieved_products.empty:\n",
    "        return []\n",
    "\n",
    "    #Create a formatted product list for AI to process\n",
    "    product_list = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"*Name:* {row['name']}\\n\"\n",
    "            f\"*Description:* {row['description']}\\n\"\n",
    "            for i, row in retrieved_products.iterrows()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #Construct the AI prompt\n",
    "    prompt = f\"\"\"\n",
    "User Query: \"{user_query}\"\n",
    "\n",
    "The following fashion products were retrieved as the most relevant matches:\n",
    "\n",
    "{product_list}\n",
    "\n",
    "Act as a fashion AI assistant. For each product, explain why it was chosen, how it matches the user's request, and provide styling tips.\n",
    "Respond with a brief and short structured paragraph for each product.\n",
    "Don't display the name of the product in your response.\n",
    "Just give your explanations as instructed.\n",
    "\"\"\"\n",
    "\n",
    "    #Generate response with Ollama\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(response)\n",
    "    response_text = response[\"message\"][\"content\"]\n",
    "\n",
    "    #Split the AI response into separate product explanations\n",
    "    explanations = response_text.split(\"\\n\\n\")  # Splitting paragraphs for individual products\n",
    "    \n",
    "    #Format structured output for the chat\n",
    "    structured_recommendations = []\n",
    "    for (index, row), explanation in zip(retrieved_products.iterrows(), explanations):\n",
    "        structured_recommendations.append(\n",
    "            {\n",
    "                \"text\": f\"**{row['name']}**\\n\\n**${row['price']}**\\n\\n{explanation[3:]}\\n\\n🔗 [View Product]({row['url']})\",\n",
    "                \"image\": row[\"image link\"],\n",
    "                \"images\": row[\"images\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return structured_recommendations  # Return structured list of text + images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: \n",
    "- Chat function for Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def chat_fashion_assistant(user_input, history):\n",
    "    \"\"\"\n",
    "    Processes user query and returns structured product recommendations with AI-generated explanations.\n",
    "    \"\"\"\n",
    "    #Get AI-generated recommendations\n",
    "    recommendations = generate_rag_response(user_input)\n",
    "\n",
    "    if not recommendations:\n",
    "        return [{\"role\": \"assistant\", \"content\": \"No matching products found.\"}]\n",
    "\n",
    "    #Create structured response list\n",
    "    response_list = []\n",
    "    for rec in recommendations:\n",
    "        images = ast.literal_eval(rec[\"images\"])\n",
    "        \n",
    "        response_list.append({\"role\": \"assistant\", \"content\": rec[\"text\"]})  # AI explanation\n",
    "        response_list.append({\"role\": \"assistant\", \"content\": gr.Gallery(images[:4], \n",
    "                                                                        columns=4, \n",
    "                                                                        rows=1, \n",
    "                                                                        object_fit=\"cover\", \n",
    "                                                                        height=\"automatic\",\n",
    "                                                                        allow_preview=True)\n",
    "                                                                        }) \n",
    "\n",
    "        response_list.append({\"role\": \"assistant\", \"content\": \"\\n\\n\\n\\n\\n\\n\"})  # Adds a horizontal divider and spacing\n",
    "\n",
    "        # response_list.append(gr.Image(rec[\"image\"]))  # Product image\n",
    "\n",
    "    return response_list  #Returning structured chat messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: \n",
    "- Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://24c2c98cc94807c43d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://24c2c98cc94807c43d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ui = gr.ChatInterface(\n",
    "    fn=chat_fashion_assistant,\n",
    "    title=\"🛍️ ShopGPT\",\n",
    "    description=\"Describe what you're looking for and get personalized recommendations!\",\n",
    "    type=\"messages\",  # Uses Gradio's structured chat format\n",
    "    theme='allenai/gradio-theme'\n",
    ")\n",
    "\n",
    "chat_ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
